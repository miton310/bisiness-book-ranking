#!/usr/bin/env python3
"""YouTube Data APIã§å…¨å‹•ç”»ã‚’å–å¾—ã—ã€æ›¸ç±æƒ…å ±ã‚’æŠ½å‡ºã—ã¦JSONã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""

import hashlib
import json
import os
import re
import sys
import time
import urllib.parse
import urllib.request

DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
CHANNELS_FILE = os.path.join(DATA_DIR, "channels.json")

YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY", "")
if not YOUTUBE_API_KEY:
    # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
    env_path = os.path.join(os.path.dirname(__file__), "..", ".env")
    if os.path.exists(env_path):
        with open(env_path) as f:
            for line in f:
                if line.startswith("YOUTUBE_API_KEY="):
                    YOUTUBE_API_KEY = line.strip().split("=", 1)[1]

AMAZON_ASSOCIATE_TAG = "miton31003"
AMAZON_TRACKING_ID = "business-book-ranking02-22"

YOUTUBE_API_BASE = "https://www.googleapis.com/youtube/v3"


# =============================================================================
# YouTube Data API
# =============================================================================

def api_get(endpoint, params):
    """YouTube Data API ã«GETãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    params["key"] = YOUTUBE_API_KEY
    url = f"{YOUTUBE_API_BASE}/{endpoint}?" + urllib.parse.urlencode(params)
    req = urllib.request.Request(url)
    with urllib.request.urlopen(req) as resp:
        return json.loads(resp.read().decode())


def get_uploads_playlist_id(channel_id):
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å†ç”Ÿãƒªã‚¹ãƒˆIDã‚’å–å¾—"""
    data = api_get("channels", {
        "part": "contentDetails",
        "id": channel_id,
    })
    items = data.get("items", [])
    if not items:
        return None
    return items[0]["contentDetails"]["relatedPlaylists"]["uploads"]


def get_all_video_ids(playlist_id):
    """å†ç”Ÿãƒªã‚¹ãƒˆã‹ã‚‰å…¨å‹•ç”»IDã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""
    video_ids = []
    page_token = None
    while True:
        params = {
            "part": "snippet",
            "playlistId": playlist_id,
            "maxResults": 50,
        }
        if page_token:
            params["pageToken"] = page_token
        data = api_get("playlistItems", params)
        for item in data.get("items", []):
            vid = item["snippet"]["resourceId"]["videoId"]
            video_ids.append(vid)
        page_token = data.get("nextPageToken")
        if not page_token:
            break
        time.sleep(0.1)
    return video_ids


def get_video_details(video_ids):
    """å‹•ç”»IDãƒªã‚¹ãƒˆã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆ50ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†ï¼‰"""
    videos = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        data = api_get("videos", {
            "part": "snippet,statistics",
            "id": ",".join(batch),
        })
        for item in data.get("items", []):
            snippet = item["snippet"]
            stats = item.get("statistics", {})
            videos.append({
                "video_id": item["id"],
                "title": snippet["title"],
                "published": snippet["publishedAt"],
                "link": f"https://www.youtube.com/watch?v={item['id']}",
                "summary": snippet.get("description", ""),
                "channel_title": snippet.get("channelTitle", ""),
                "view_count": int(stats.get("viewCount", 0)),
                "like_count": int(stats.get("likeCount", 0)),
            })
        time.sleep(0.1)
    return videos


def fetch_all_channel_videos(channel_id):
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®å…¨å‹•ç”»ã‚’å–å¾—"""
    playlist_id = get_uploads_playlist_id(channel_id)
    if not playlist_id:
        print(f"  [ERROR] ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å†ç”Ÿãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []
    video_ids = get_all_video_ids(playlist_id)
    print(f"  å‹•ç”»IDå–å¾—: {len(video_ids)}ä»¶")
    videos = get_video_details(video_ids)
    print(f"  å‹•ç”»è©³ç´°å–å¾—: {len(videos)}ä»¶")
    return videos


# =============================================================================
# æ›¸ç±æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
# =============================================================================

def extract_book_info_list(summary):
    """æ¦‚è¦æ¬„ã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°å†Šå¯¾å¿œï¼‰"""
    results = []

    amazon_urls = re.findall(r'https?://amzn\.to/[A-Za-z0-9]+', summary)

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æœ¬è¦ç´„ãƒãƒ£ãƒ³ãƒãƒ« / ã‚µãƒ©ã‚¿ãƒ¡ã•ã‚“ã€Œã‚¿ã‚¤ãƒˆãƒ«ï¼šã€ã€Œè‘—è€…ï¼šã€ã€Œå‡ºç‰ˆç¤¾ï¼šã€
    title_match = re.search(r'ã‚¿ã‚¤ãƒˆãƒ«[ï¼š:](.+)', summary)
    if title_match:
        info = {
            "title": title_match.group(1).strip(),
            "author": None,
            "publisher": None,
        }
        author_match = re.search(r'è‘—è€…[ï¼š:](.+)', summary)
        if author_match:
            info["author"] = author_match.group(1).strip()
        publisher_match = re.search(r'å‡ºç‰ˆç¤¾[ï¼š:](.+)', summary)
        if publisher_match:
            info["publisher"] = publisher_match.group(1).strip()
        results.append(info)
        return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ•ã‚§ãƒ«ãƒŸæ¼«ç”»å¤§å­¦ã€Œå‚è€ƒï¼šæ›¸å è‘—è€…å ã•ã¾ã€
    ref_match = re.search(r'å‚è€ƒ[ï¼š:](.+?)(?:\s+ã•ã¾|\s*$)', summary, re.MULTILINE)
    if ref_match:
        results.append({
            "title": ref_match.group(1).strip(),
            "author": None,
            "publisher": None,
        })
        return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: å­¦è­˜ã‚µãƒ­ãƒ³ã€Œã€amazonãƒªãƒ³ã‚¯ã€‘\nã€æ›¸åã€è‘—è€… / å‡ºç‰ˆç¤¾ã€
    if "ã€amazonãƒªãƒ³ã‚¯ã€‘" in summary:
        gakushiki_match = re.search(r'ã€(.+?)ã€(.+?)(?:\s*/\s*(.+))?$', summary, re.MULTILINE)
        if gakushiki_match:
            info = {
                "title": gakushiki_match.group(1).strip(),
                "author": None,
                "publisher": None,
            }
            if gakushiki_match.group(2):
                info["author"] = gakushiki_match.group(2).strip()
            if gakushiki_match.group(3):
                info["publisher"] = gakushiki_match.group(3).strip()
            results.append(info)
            return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ã‚¢ãƒã‚¿ãƒ­ãƒ¼ã€Œæ›¸ç±ã®è³¼å…¥ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    abataro_section = re.search(
        r'(?:ã€æ›¸ç±ã®è³¼å…¥ã€‘|â–¼æ›¸ç±ã®è³¼å…¥)\s*\n?(.*?)(?=\nâ–¼|\n\n\n|\Z)', summary, re.DOTALL
    )
    if abataro_section:
        section_text = abataro_section.group(1)
        lines = section_text.strip().split('\n')
        seen_titles = set()
        is_first = True
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            # éæ›¸ç±è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if not line or line.startswith('http') or 'ã‚¨ãƒƒã‚»ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ' in line or 'ç°¡æ˜“ç‰ˆ' in line:
                i += 1
                continue
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ã‚µãƒ¼ãƒ“ã‚¹å®£ä¼ãƒ»ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãƒ»çµµæ–‡å­—ä»˜ãå‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if (line.startswith('ã€') or line.startswith('#') or
                'Audible' in line or 'Kindle' in line or 'amzn.to' in line or
                line.startswith('ğŸ“—') or line.startswith('ğŸ“•') or
                'æœ¬ã‚’è´ã' in line or 'é–¢é€£å‹•ç”»' in line or
                'åˆ†è§£èª¬' in line or 'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²' in line or
                'SNS' in line or 'Twitter' in line or 'Instagram' in line):
                i += 1
                continue
            line = re.sub(r'^ãƒ»\s*', '', line)
            book_match = re.match(r'(.+?)(?:[ï½œ|](.+?))?(?:[ï¼ˆ(](.+?)[ï¼‰)])?$', line)
            if book_match:
                title = book_match.group(1).strip()
                author = book_match.group(2).strip() if book_match.group(2) else None
                publisher = book_match.group(3).strip() if book_match.group(3) else None
                if title not in seen_titles:
                    seen_titles.add(title)
                    results.append({
                        "title": title,
                        "author": author,
                        "publisher": publisher,
                        "_is_first": is_first,
                    })
                is_first = False
            i += 1
        if results:
            return results

    return results


def generate_amazon_search_url(book_title):
    """æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰Amazonæ¤œç´¢URLã‚’ç”Ÿæˆï¼ˆã‚¢ã‚½ã‚·ã‚¨ã‚¤ãƒˆã‚¿ã‚°ä»˜ãï¼‰"""
    query = urllib.parse.quote(book_title)
    return f"https://www.amazon.co.jp/s?k={query}&i=stripbooks&tag={AMAZON_ASSOCIATE_TAG}&linkId={AMAZON_TRACKING_ID}"


def generate_book_id(title):
    """æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ç”Ÿæˆ"""
    return hashlib.md5(title.encode()).hexdigest()[:12]


# =============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =============================================================================

def load_channels():
    with open(CHANNELS_FILE, "r", encoding="utf-8") as f:
        return json.load(f)["channels"]


def main():
    if not YOUTUBE_API_KEY:
        print("ERROR: YOUTUBE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    os.makedirs(DATA_DIR, exist_ok=True)
    channels = load_channels()
    all_books = {}

    for ch in channels:
        channel_name = ch["name"]
        channel_id = ch["channel_id"]
        print(f"\n=== {channel_name} (ID: {channel_id}) ===")

        videos = fetch_all_channel_videos(channel_id)

        for video in videos:
            summary = video.get("summary", "")
            book_info_list = extract_book_info_list(summary)

            if not book_info_list:
                continue

            for book_info in book_info_list:
                book_title = book_info.get("title")
                if not book_title:
                    continue
                # è‡ªè‘—å®£ä¼ã‚¹ã‚­ãƒƒãƒ—
                if book_info.get("_is_first") and len(book_info_list) > 1:
                    continue

                amazon_url = generate_amazon_search_url(book_title)

                if book_title not in all_books:
                    all_books[book_title] = {
                        "id": generate_book_id(book_title),
                        "title": book_title,
                        "author": book_info.get("author"),
                        "publisher": book_info.get("publisher"),
                        "amazon_url": amazon_url,
                        "count": 0,
                        "total_views": 0,
                        "total_likes": 0,
                        "videos": [],
                    }

                all_books[book_title]["count"] += 1
                all_books[book_title]["total_views"] += video.get("view_count", 0)
                all_books[book_title]["total_likes"] += video.get("like_count", 0)
                all_books[book_title]["videos"].append({
                    "video_id": video["video_id"],
                    "video_title": video["title"],
                    "channel": channel_name,
                    "link": video["link"],
                    "published": video["published"],
                    "view_count": video.get("view_count", 0),
                    "like_count": video.get("like_count", 0),
                })

    # --- çµæœè¡¨ç¤º ---
    books_list = list(all_books.values())
    print(f"\n=== æŠ½å‡ºçµæœ ===")
    print(f"æ›¸ç±æ•°: {len(books_list)}")

    # --- JSONç”Ÿæˆ ---

    # books.jsonï¼ˆç´¹ä»‹å›æ•°é †ï¼‰
    books_by_count = sorted(books_list, key=lambda x: x["count"], reverse=True)
    with open(os.path.join(DATA_DIR, "books.json"), "w", encoding="utf-8") as f:
        json.dump(books_by_count, f, ensure_ascii=False, indent=2)

    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”¨ã®è»½é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    def make_ranking_entry(book):
        return {
            "id": book["id"],
            "title": book["title"],
            "author": book.get("author"),
            "count": book["count"],
            "total_views": book["total_views"],
            "total_likes": book["total_likes"],
            "amazon_url": book["amazon_url"],
        }

    # rankings.jsonï¼ˆç´¹ä»‹å›æ•°é †ï¼‰
    rankings_count = [make_ranking_entry(b) for b in books_by_count]
    with open(os.path.join(DATA_DIR, "rankings.json"), "w", encoding="utf-8") as f:
        json.dump(rankings_count, f, ensure_ascii=False, indent=2)

    # rankings_views.jsonï¼ˆå†ç”Ÿå›æ•°åˆè¨ˆé †ï¼‰
    books_by_views = sorted(books_list, key=lambda x: x["total_views"], reverse=True)
    rankings_views = [make_ranking_entry(b) for b in books_by_views]
    with open(os.path.join(DATA_DIR, "rankings_views.json"), "w", encoding="utf-8") as f:
        json.dump(rankings_views, f, ensure_ascii=False, indent=2)

    # rankings_likes.jsonï¼ˆã„ã„ã­åˆè¨ˆé †ï¼‰
    books_by_likes = sorted(books_list, key=lambda x: x["total_likes"], reverse=True)
    rankings_likes = [make_ranking_entry(b) for b in books_by_likes]
    with open(os.path.join(DATA_DIR, "rankings_likes.json"), "w", encoding="utf-8") as f:
        json.dump(rankings_likes, f, ensure_ascii=False, indent=2)

    print(f"\n--- TOP20ï¼ˆç´¹ä»‹å›æ•°é †ï¼‰---")
    for i, book in enumerate(books_by_count[:20], 1):
        print(f"  {i}. ã€{book['title']}ã€ (ç´¹ä»‹{book['count']}å› / å†ç”Ÿ{book['total_views']:,} / ã„ã„ã­{book['total_likes']:,})")

    print(f"\n--- TOP10ï¼ˆå†ç”Ÿå›æ•°é †ï¼‰---")
    for i, book in enumerate(books_by_views[:10], 1):
        print(f"  {i}. ã€{book['title']}ã€ (å†ç”Ÿ{book['total_views']:,} / ç´¹ä»‹{book['count']}å›)")

    print(f"\n--- TOP10ï¼ˆã„ã„ã­é †ï¼‰---")
    for i, book in enumerate(books_by_likes[:10], 1):
        print(f"  {i}. ã€{book['title']}ã€ (ã„ã„ã­{book['total_likes']:,} / ç´¹ä»‹{book['count']}å›)")

    print(f"\nãƒ‡ãƒ¼ã‚¿ã‚’ {DATA_DIR} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()
