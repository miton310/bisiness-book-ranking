#!/usr/bin/env python3
"""YouTube Data APIã§å…¨å‹•ç”»ã‚’å–å¾—ã—ã€æ›¸ç±æƒ…å ±ã‚’æŠ½å‡ºã—ã¦JSONã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""

import hashlib
import json
import os
import re
import sys
import time
import urllib.parse
import urllib.request

# Amazonãƒªãƒ³ã‚¯ã‹ã‚‰æ›¸ç±æƒ…å ±å–å¾—
from fetch_amazon_info import extract_books_from_amazon_links

DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
CHANNELS_FILE = os.path.join(DATA_DIR, "channels.json")

YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY", "")
if not YOUTUBE_API_KEY:
    # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
    env_path = os.path.join(os.path.dirname(__file__), "..", ".env")
    if os.path.exists(env_path):
        with open(env_path) as f:
            for line in f:
                if line.startswith("YOUTUBE_API_KEY="):
                    YOUTUBE_API_KEY = line.strip().split("=", 1)[1]

AMAZON_ASSOCIATE_TAG = "miton31003"
AMAZON_TRACKING_ID = "business-book-ranking02-22"

YOUTUBE_API_BASE = "https://www.googleapis.com/youtube/v3"


# =============================================================================
# YouTube Data API
# =============================================================================

def api_get(endpoint, params):
    """YouTube Data API ã«GETãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    params["key"] = YOUTUBE_API_KEY
    url = f"{YOUTUBE_API_BASE}/{endpoint}?" + urllib.parse.urlencode(params)
    req = urllib.request.Request(url)
    with urllib.request.urlopen(req) as resp:
        return json.loads(resp.read().decode())


def get_uploads_playlist_id(channel_id):
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å†ç”Ÿãƒªã‚¹ãƒˆIDã‚’å–å¾—"""
    data = api_get("channels", {
        "part": "contentDetails",
        "id": channel_id,
    })
    items = data.get("items", [])
    if not items:
        return None
    return items[0]["contentDetails"]["relatedPlaylists"]["uploads"]


def get_all_video_ids(playlist_id):
    """å†ç”Ÿãƒªã‚¹ãƒˆã‹ã‚‰å…¨å‹•ç”»IDã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""
    video_ids = []
    page_token = None
    while True:
        params = {
            "part": "snippet",
            "playlistId": playlist_id,
            "maxResults": 50,
        }
        if page_token:
            params["pageToken"] = page_token
        data = api_get("playlistItems", params)
        for item in data.get("items", []):
            vid = item["snippet"]["resourceId"]["videoId"]
            video_ids.append(vid)
        page_token = data.get("nextPageToken")
        if not page_token:
            break
        time.sleep(0.1)
    return video_ids


def get_video_details(video_ids):
    """å‹•ç”»IDãƒªã‚¹ãƒˆã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆ50ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†ï¼‰"""
    videos = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        data = api_get("videos", {
            "part": "snippet,statistics",
            "id": ",".join(batch),
        })
        for item in data.get("items", []):
            snippet = item["snippet"]
            stats = item.get("statistics", {})
            videos.append({
                "video_id": item["id"],
                "title": snippet["title"],
                "published": snippet["publishedAt"],
                "link": f"https://www.youtube.com/watch?v={item['id']}",
                "summary": snippet.get("description", ""),
                "channel_title": snippet.get("channelTitle", ""),
                "view_count": int(stats.get("viewCount", 0)),
                "like_count": int(stats.get("likeCount", 0)),
            })
        time.sleep(0.1)
    return videos


def fetch_all_channel_videos(channel_id):
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®å…¨å‹•ç”»ã‚’å–å¾—"""
    playlist_id = get_uploads_playlist_id(channel_id)
    if not playlist_id:
        print(f"  [ERROR] ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å†ç”Ÿãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []
    video_ids = get_all_video_ids(playlist_id)
    print(f"  å‹•ç”»IDå–å¾—: {len(video_ids)}ä»¶")
    videos = get_video_details(video_ids)
    print(f"  å‹•ç”»è©³ç´°å–å¾—: {len(videos)}ä»¶")
    return videos


# =============================================================================
# æ›¸ç±æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
# =============================================================================

def extract_book_info_list(summary, video_title=None):
    """æ¦‚è¦æ¬„ãƒ»å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’æŠ½å‡º"""
    results = []

    # ãƒ‘ã‚¿ãƒ¼ãƒ³0: å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æŠ½å‡ºã€Œã€è¦ç´„ã€‘ã‚¿ã‚¤ãƒˆãƒ«ã€è‘—è€…ã€‘ã€ï¼ˆãƒ•ã‚§ãƒ«ãƒŸæ¼«ç”»å¤§å­¦ç­‰ï¼‰
    if video_title:
        m = re.match(r'ã€(?:è¦ç´„|æ¼«ç”»)ã€‘(.+?)ã€(.+?)ã€‘', video_title)
        if m:
            book_title = m.group(1).strip()
            author = m.group(2).strip()
            results.append({
                "title": book_title,
                "author": author,
                "publisher": None,
            })
            return results

    # TODO: Amazonãƒªãƒ³ã‚¯ã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’å–å¾—ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼‰
    # amazon_urls = re.findall(r'https?://amzn\.to/[A-Za-z0-9]+', summary)
    # if amazon_urls:
    #     amazon_books = extract_books_from_amazon_links(amazon_urls, max_books=5, context=summary)
    #     for book in amazon_books:
    #         results.append({
    #             "title": book["title"],
    #             "author": None,
    #             "publisher": None,
    #             "amazon_url": book["amazon_url"],
    #         })
    #     if results:
    #         return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æœ¬è¦ç´„ãƒãƒ£ãƒ³ãƒãƒ« / ã‚µãƒ©ã‚¿ãƒ¡ã•ã‚“ã€Œã‚¿ã‚¤ãƒˆãƒ«ï¼šã€ã€Œè‘—è€…ï¼šã€ã€Œå‡ºç‰ˆç¤¾ï¼šã€
    title_match = re.search(r'ã‚¿ã‚¤ãƒˆãƒ«[ï¼š:](.+)', summary)
    if title_match:
        info = {
            "title": title_match.group(1).strip(),
            "author": None,
            "publisher": None,
        }
        author_match = re.search(r'è‘—è€…[ï¼š:](.+)', summary)
        if author_match:
            info["author"] = author_match.group(1).strip()
        publisher_match = re.search(r'å‡ºç‰ˆç¤¾[ï¼š:](.+)', summary)
        if publisher_match:
            info["publisher"] = publisher_match.group(1).strip()
        results.append(info)
        return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ•ã‚§ãƒ«ãƒŸæ¼«ç”»å¤§å­¦ã€Œå‚è€ƒï¼šæ›¸å è‘—è€…å ã•ã¾ã€
    # ã€Œå‚è€ƒæ–‡çŒ®ï¼šã€ã‚‚å¯¾å¿œ
    ref_match = re.search(r'å‚è€ƒ(?:æ–‡çŒ®)?[ï¼š:](.+?)(?:\s+ã•ã¾|\s*$)', summary, re.MULTILINE)
    if ref_match:
        title_text = ref_match.group(1).strip()
        # è‘—è€…åã ã‘ã®è¡Œã‚’é™¤å¤–ï¼ˆã€Œã•ã¾ã€ã§çµ‚ã‚ã‚‹äººåã®ã¿ã€æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ãªã—ï¼‰
        if not re.match(r'^[\w\sãƒ»ã€€]+ã•ã¾', title_text) and title_text:
            results.append({
                "title": title_text,
                "author": None,
                "publisher": None,
            })
            return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: å­¦è­˜ã‚µãƒ­ãƒ³ã€Œã€amazonãƒªãƒ³ã‚¯ã€‘\nã€æ›¸åã€è‘—è€… / å‡ºç‰ˆç¤¾ã€
    if "ã€amazonãƒªãƒ³ã‚¯ã€‘" in summary:
        gakushiki_match = re.search(r'ã€(.+?)ã€(.+?)(?:\s*/\s*(.+))?$', summary, re.MULTILINE)
        if gakushiki_match:
            info = {
                "title": gakushiki_match.group(1).strip(),
                "author": None,
                "publisher": None,
            }
            if gakushiki_match.group(2):
                info["author"] = gakushiki_match.group(2).strip()
            if gakushiki_match.group(3):
                info["publisher"] = gakushiki_match.group(3).strip()
            results.append(info)
            return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ã‚µãƒ ã®æœ¬è§£èª¬chã€Œã€ä»Šå›ã®å‚è€ƒæ›¸ç±ğŸ“šã€‘ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    sam_section = re.search(
        r'ã€ä»Šå›ã®å‚è€ƒæ›¸ç±.*?ã€‘\s*\n(.*?)(?=ã€|$)', summary, re.DOTALL
    )
    if sam_section:
        section_text = sam_section.group(1).strip()
        lines = section_text.split('\n')
        title_line = None
        author_line = None
        for line in lines:
            line = line.strip()
            if not line or line.startswith('http'):
                continue
            # è‘—è€…è¡Œã‚’åˆ¤å®š: ã€Œã€œ(è‘—)ã€ã€Œã€œï¼ˆè‘—ï¼‰ã€ã‚’å«ã‚€è¡Œ
            if re.search(r'[ï¼ˆ(]è‘—[ï¼‰)]', line):
                author_line = line
            elif not title_line:
                # æœ€åˆã®éè‘—è€…è¡Œã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦å–å¾—
                title_line = re.sub(r'\s*(Kindleç‰ˆ|å˜è¡Œæœ¬|æ–‡åº«|æ–°æ›¸|ãƒãƒ¼ãƒ‰ã‚«ãƒãƒ¼)\s*$', '', line).strip()
                # å…ˆé ­ã®ã€Œãƒ»ã€ã‚’é™¤å»
                title_line = re.sub(r'^[ãƒ»ï½¥]', '', title_line).strip()
        if title_line:
            info = {"title": title_line, "author": None, "publisher": None}
            if author_line:
                author_match = re.match(r'(.+?)\s*[ï¼ˆ(]è‘—[ï¼‰)]', author_line)
                if author_match:
                    info["author"] = author_match.group(1).strip()
                pub_match = re.search(r'([^\s]+?)[ï¼ˆ(]ç·¨é›†[ï¼‰)]', author_line)
                if pub_match:
                    info["publisher"] = pub_match.group(1).strip()
            results.append(info)
            return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³5: PIVOTç³»ã€Œï¼œå‚è€ƒæ›¸ç±ï¼ã€ã€Œâ–¼å‚è€ƒæ›¸ç±ã€ã€Œâ–¼é–¢é€£æ›¸ç±ã€ã€Œâ–¼æœ¬æ˜ åƒã§ç´¹ä»‹ã—ãŸæ›¸ç±ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    pivot_section = re.search(
        r'(?:[ï¼œ<]å‚è€ƒæ›¸ç±[ï¼>]|â–¼å‚è€ƒæ›¸ç±|â–¼é–¢é€£æ›¸ç±|â–¼æœ¬æ˜ åƒã§ç´¹ä»‹ã—ãŸæ›¸ç±)\s*\n(.*?)(?=\n[ï¼œ<]|\nâ–¼[^å‚é–¢æœ¬]|\n[â– â—]|\nâ€»|\n\n\n|$)', summary, re.DOTALL
    )
    if pivot_section:
        section_text = pivot_section.group(1).strip()
        lines = section_text.split('\n')
        for line in lines:
            line = line.strip()
            if not line or line.startswith('http') or line.startswith('â€»'):
                continue

            title = None
            author = None

            # ãƒ‘ã‚¿ãƒ¼ãƒ³A: ã€ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚’å„ªå…ˆï¼ˆå†…éƒ¨ã«ã€Œã€ãŒå«ã¾ã‚Œã¦ã‚‚OKï¼‰
            book_match = re.search(r'ã€(.+?)ã€', line)
            if book_match:
                title = book_match.group(1).strip()
                before = line[:book_match.start()].strip()
                if before:
                    author = before
                after = line[book_match.end():].strip()
                if not author and after:
                    a_match = re.match(r'(.+?)\s*[ï¼ˆ(]è‘—[ï¼‰)]', after)
                    if a_match:
                        author = a_match.group(1).strip()

            # ãƒ‘ã‚¿ãƒ¼ãƒ³B: ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ï¼‹å¾Œç¶šãƒ†ã‚­ã‚¹ãƒˆã‚‚å«ã‚ã‚‹
            if not title:
                book_match = re.search(r'ã€Œ(.+?)ã€(.+?)(?=[ï¼ˆ(]|https?://|\s*$)', line)
                if book_match:
                    # ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ã®å¾Œã‚ã‚‚ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸€éƒ¨ã¨ã—ã¦çµåˆ
                    title = book_match.group(1).strip() + book_match.group(2).strip()
                    # æœ«å°¾ã®æ‹¬å¼§å†…ï¼ˆå‡ºç‰ˆç¤¾ç­‰ï¼‰ã‚’é™¤å»
                    title = re.sub(r'[ï¼ˆ(][^ï¼‰)]+[ï¼‰)]$', '', title).strip()

            if not title:
                continue

            results.append({
                "title": title,
                "author": author,
                "publisher": None,
            })
        # PIVOTã®å‚è€ƒæ›¸ç±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹å ´åˆã¯çµæœã«é–¢ã‚ã‚‰ãšã“ã“ã§è¿”ã™
        # ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³6ã®amzn.toæ±ç”¨æŠ½å‡ºã«è½ã¡ãªã„ã‚ˆã†ã«ã™ã‚‹ï¼‰
        return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³6: ä¸ƒç€¬ã‚¢ãƒªãƒ¼ã‚µ â€” amzn.toãƒªãƒ³ã‚¯ã‹ã‚‰æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
    # å½¢å¼A: ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€€https://amzn.to/xxxã€(åŒä¸€è¡Œ)
    # å½¢å¼B: ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€+ æ¬¡è¡Œã€Œhttps://amzn.to/xxxã€(åˆ¥è¡Œ)
    amazon_lines = re.findall(r'https?://amzn\.to/[A-Za-z0-9]+', summary)
    if amazon_lines:
        lines = summary.split('\n')
        ng_words = ['Amazon', 'URL', 'ãƒªãƒ³ã‚¯', 'ä¸ƒç€¬', 'å•†å“ç´¹ä»‹', 'ç‰¹å…¸',
                    'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚«ãƒ¼ãƒ‰', 'Success Book', 'å‹•ç”»', 'æ¦‚è¦æ¬„',
                    'ãŠã™ã™ã‚é †ã§ã¯ãªã„', 'ã‚¢ã‚½ã‚·ã‚¨ã‚¤ãƒˆ', 'è³¼å…¥ãƒšãƒ¼ã‚¸',
                    'æä¾›:', 'Mainichi Eikaiwa', 'è©•åˆ¤', 'ãŠã™ã™ã‚æœ¬', 'å‡ºæ¼”æœ¬',
                    'å‚è€ƒæœ¬', 'ãŠå‹§ã‚æœ¬', 'TOEIC', 'å‹‰å¼·æœ¬', 'ã‚ªãƒ¼ãƒ‡ã‚£ãƒ–ãƒ«',
                    'Audible', 'Kindle', 'Udemy', 'æ‰‹å¸³', 'ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼',
                    'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è‹±ä¼šè©±', 'AQUES', 'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²', 'LOWYAã®',
                    'Meta Quest', 'Kindleç«¯æœ«', 'æœ¬æ£šãƒ‡ã‚¹ã‚¯', 'ã¯ã“ã¡ã‚‰',
                    'ã‚¿ã‚¤ãƒãƒ¼', 'ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼', 'ãƒœãƒ¼ãƒ‰ã‚²ãƒ¼ãƒ ', 'ã‹ã£ã•',
                    'ãƒ†ãƒ©ãƒ˜ãƒ«ãƒ„', 'ã‚¤ãƒ¤ãƒ›ãƒ³', 'ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰', 'ãƒã‚¦ã‚¹',
                    'ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤', 'ãƒ¢ãƒ‹ã‚¿ãƒ¼', 'ãƒã‚§ã‚¢', 'ãƒ©ã‚¤ãƒˆä»˜ã',
                    'é‡‘ãƒ•ãƒ¬', 'ã‚­ã‚¯ã‚¿ãƒ³', 'ã§ã‚‹1000å•', 'å…¬å¼å•é¡Œé›†',
                    'ç²¾é¸å•é¡Œé›†', 'ç²¾é¸æ¨¡è©¦']

        for i, line in enumerate(lines):
            line_stripped = line.strip()
            amazon_match = re.search(r'https?://amzn\.to/[A-Za-z0-9]+', line_stripped)
            if not amazon_match:
                continue

            title_candidate = None
            amazon_url = amazon_match.group(0)

            # å½¢å¼A: amzn.toã®å‰ã«ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹ï¼ˆåŒä¸€è¡Œï¼‰
            before_url = line_stripped[:amazon_match.start()].strip()
            if before_url and not before_url.startswith('http'):
                title_candidate = before_url
            # å½¢å¼B: amzn.toã ã‘ã®è¡Œ â†’ å‰ã®è¡ŒãŒã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè‘—è€…ãƒ»å‡ºç‰ˆç¤¾è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            elif line_stripped == amazon_url and i > 0:
                for j in range(i-1, max(i-5, -1), -1):
                    prev_line = lines[j].strip()
                    if not prev_line or prev_line.startswith('http'):
                        break
                    # è‘—è€…ãƒ»å‡ºç‰ˆç¤¾ãªã©ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç©ºç™½å…¥ã‚Šã‚‚å¯¾å¿œ: ã€Œè‘—ã€€è€…ã€ã€Œç›£ã€€è¨³ã€ï¼‰
                    if re.match(r'^(è‘—[\sã€€]*è€…|ç›£[\sã€€]*è¨³|å‡ºç‰ˆç¤¾|å‡ºç‰ˆ|ç™ºè¡Œ|ç™ºå£²æ—¥|ä¾¡æ ¼|å®šä¾¡)[\s\u200f\u200e]*[ï¼š:.\sã€€]', prev_line):
                        continue
                    # æ‹¬å¼§ã ã‘ã®è£œè¶³è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä¾‹: ã€Œ(æ—¥æœ¬èªç‰ˆ)ã€ã€Œï¼ˆå®Œå…¨ç‰ˆï¼‰ã€ï¼‰
                    if re.match(r'^[ï¼ˆ(].+[ï¼‰)]$', prev_line):
                        continue
                    # è‘—è€…è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä¾‹: ã€Œã‚¨ãƒŸãƒ³ãƒ»ãƒ¦ãƒ«ãƒã‚º (è‘—)ã€ï¼‰
                    if re.search(r'[ï¼ˆ(]è‘—[ï¼‰)]', prev_line) and 'ã€' not in prev_line and 'ã€Œ' not in prev_line:
                        continue
                    title_candidate = prev_line
                    break

            if not title_candidate:
                continue

            # NGãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            if any(ng in title_candidate for ng in ng_words):
                continue

            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            cleaned = re.sub(r'^[*\sãƒ»â€»â¤ï¸ğŸ“•ğŸ“—ğŸ“˜ğŸ“™ğŸ”½â–½â†“]+', '', title_candidate).strip()
            # æ‹¬å¼§ä»˜ãã®è£œè¶³ã‚’é™¤å»: ã€Œã‚¿ã‚¤ãƒˆãƒ«(Amazon)ã€â†’ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€
            cleaned = re.sub(r'[ï¼ˆ(](?:Amazon|Amazonãƒªãƒ³ã‚¯|ã‚¢ãƒã‚¾ãƒ³)[ï¼‰)]$', '', cleaned).strip()
            # ã€ã€ã€Œã€ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯å¤–ã™
            if cleaned.startswith('ã€') and cleaned.endswith('ã€'):
                cleaned = cleaned[1:-1]
            if cleaned.startswith('ã€Œ') and cleaned.endswith('ã€'):
                cleaned = cleaned[1:-1]

            if cleaned and len(cleaned) > 2:
                results.append({
                    "title": cleaned,
                    "author": None,
                    "publisher": None,
                })

        if results:
            return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³5: ã‚¢ãƒã‚¿ãƒ­ãƒ¼ã€Œæ›¸ç±ã®è³¼å…¥ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    abataro_section = re.search(
        r'(?:ã€æ›¸ç±ã®è³¼å…¥ã€‘|â–¼æ›¸ç±ã®è³¼å…¥)\s*\n?(.*?)(?=\nâ–¼|\n\n\n|\Z)', summary, re.DOTALL
    )
    if abataro_section:
        section_text = abataro_section.group(1)
        lines = section_text.strip().split('\n')
        seen_titles = set()
        is_first = True
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            # éæ›¸ç±è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if not line or line.startswith('http') or 'ã‚¨ãƒƒã‚»ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ' in line or 'ç°¡æ˜“ç‰ˆ' in line:
                i += 1
                continue
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ã‚µãƒ¼ãƒ“ã‚¹å®£ä¼ãƒ»ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãƒ»çµµæ–‡å­—ä»˜ãå‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if (line.startswith('ã€') or line.startswith('#') or
                'Audible' in line or 'Kindle' in line or 'amzn.to' in line or
                line.startswith('ğŸ“—') or line.startswith('ğŸ“•') or
                'æœ¬ã‚’è´ã' in line or 'é–¢é€£å‹•ç”»' in line or
                'åˆ†è§£èª¬' in line or 'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²' in line or
                'SNS' in line or 'Twitter' in line or 'Instagram' in line or
                'OUTPUTèª­æ›¸è¡“' in line):
                i += 1
                continue
            line = re.sub(r'^ãƒ»\s*', '', line)
            book_match = re.match(r'(.+?)(?:[ï½œ|](.+?))?(?:[ï¼ˆ(](.+?)[ï¼‰)])?$', line)
            if book_match:
                title = book_match.group(1).strip()
                author = book_match.group(2).strip() if book_match.group(2) else None
                publisher = book_match.group(3).strip() if book_match.group(3) else None
                if title not in seen_titles:
                    seen_titles.add(title)
                    results.append({
                        "title": title,
                        "author": author,
                        "publisher": publisher,
                        "_is_first": is_first,
                    })
                is_first = False
            i += 1
        if results:
            return results

    return results


def clean_book_title(title):
    """ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰è‘—è€…åãƒ»å‡ºç‰ˆç¤¾ãªã©ã®ä»˜åŠ æƒ…å ±ã‚’é™¤å»"""
    if not title:
        return title
    title = title.strip()

    # å…ˆé ­ã®çµµæ–‡å­—ãƒ»è¨˜å·ãƒ»ä¸¸æ•°å­—ã‚’é™¤å»ï¼ˆğŸ“šğŸ“—â–¶ï¸â—‰â‘ â‘¡ç­‰ï¼‰
    # U+FE0E/U+FE0F (variation selector) ã‚‚å«ã‚ã¦é™¤å»
    title = re.sub(r'^[ğŸ“šğŸ“—ğŸ“•ğŸ“˜ğŸ“™ğŸ“–ğŸ”½â–¶â–·â—‰â—â—‹â—â– â–¡â–ªâ–«â˜…â˜†âœ…âœ“â†’â–ºâ¤ğŸ”¶ğŸ”·ğŸ’¡ğŸ¯ğŸ“Œâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³\ufe0e\ufe0f]+[\sã€€.ï¼‰)ã€]*', '', title)

    # ã€Œæ›¸ç±ï¼šã€ã€Œè‘—æ›¸ï¼šã€ç­‰ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
    title = re.sub(r'^(æ›¸ç±|è‘—æ›¸)[ï¼š:]\s*', '', title)

    # ã€ã‚¿ã‚¤ãƒˆãƒ«ã€ï¼‹å¾Œç¶šãƒ†ã‚­ã‚¹ãƒˆ â†’ ã€ã€å†…ã ã‘æŠ½å‡º
    m = re.search(r'ã€(.+?)ã€', title)
    if m:
        inner = m.group(1).strip()
        before = title[:m.start()].strip()
        after = title[m.end():].strip()
        if before:
            return inner
        if after:
            return inner

    # ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ï¼‹å¾Œç¶šãƒ†ã‚­ã‚¹ãƒˆ â†’ ã€Œã€å†…ã ã‘æŠ½å‡º
    m = re.search(r'ã€Œ(.+?)ã€', title)
    if m:
        inner = m.group(1).strip()
        before = title[:m.start()].strip()
        after = title[m.end():].strip()
        # å‰ã«ã€Œè‘—æ›¸ã€ã€Œè‘—è€…ã€ç­‰ã€å¾Œã‚ã«â–·ã‚„(è‘—)ç­‰ãŒã‚ã‚‹å ´åˆ
        if re.match(r'^(è‘—æ›¸|è‘—è€…)', before) or (after and re.match(r'[â–·â–¶â†’(ï¼ˆ]', after)):
            return inner

    # æœ«å°¾ã®ã€Œï¼ˆè‘—è€…åè‘—ï¼‰ã€ã€Œ(è‘—è€…åè‘—)ã€ã‚’é™¤å»
    title = re.sub(r'[ï¼ˆ(].+?è‘—[ï¼‰)]\s*$', '', title).strip()

    # æœ«å°¾ã®ã€Œï¼ˆå‡ºç‰ˆç¤¾åï¼‰ã€ã¨å¾Œç¶šã®è‘—è€…åç­‰ã‚’é™¤å»ï¼ˆæ–‡åº«ãƒ»æ–°æ›¸ãƒ»é¸æ›¸ãªã©ï¼‰
    title = re.sub(r'[ï¼ˆ(](å¹»å†¬èˆæ–‡åº«|æ–°æ½®æ–°æ›¸|è¬›è«‡ç¤¾æ–‡åº«|è§’å·æ–‡åº«|æ–‡æ˜¥æ–‡åº«|é›†è‹±ç¤¾æ–‡åº«|PHPæ–°æ›¸|ä¸­å…¬æ–°æ›¸|å²©æ³¢æ–°æ›¸|ã¡ãã¾æ–°æ›¸|å…‰æ–‡ç¤¾æ–°æ›¸|æœæ—¥æ–°æ›¸|SBæ–°æ›¸|ç¥¥ä¼ç¤¾æ–°æ›¸|è¬›è«‡ç¤¾ç¾ä»£æ–°æ›¸|è¬›è«‡ç¤¾\+Î±æ–°æ›¸|ãƒãƒ¤ã‚«ãƒ¯æ–‡åº«|å‰µå…ƒæ¨ç†æ–‡åº«|PHPæ–‡åº«|ã ã„ã‚æ–‡åº«|çŸ¥çš„ç”Ÿãã‹ãŸæ–‡åº«|ä¸‰ç¬ æ›¸æˆ¿)[ï¼‰)].*$', '', title).strip()

    # ã€Œæ¸¡é‚‰æ­£è£• è‘—ã€ã‚¿ã‚¤ãƒˆãƒ«ã€ã€ãƒ‘ã‚¿ãƒ¼ãƒ³
    m = re.match(r'.+?\s+è‘—\s*ã€(.+?)ã€', title)
    if m:
        return m.group(1).strip()

    return title


def is_valid_book_title(title):
    """æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦æœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    if not title or not isinstance(title, str):
        return False

    title = title.strip()

    # çŸ­ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’é™¤å¤–ï¼ˆ3æ–‡å­—ä»¥ä¸‹ï¼‰
    if len(title) <= 3:
        return False

    # çµµæ–‡å­—ã§å§‹ã¾ã‚‹ã‚‚ã®ã‚’é™¤å¤–
    emoji_starts = ['ğŸ“š', 'ğŸ“—', 'ğŸ“•', 'ğŸ“˜', 'ğŸ“™', 'â–¼', 'ã€', 'â– ', 'â—', 'â—‰', 'â—', 'â—‹', 'ãƒ»', 'â€»']
    if any(title.startswith(emoji) for emoji in emoji_starts):
        return False

    # NGãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã‚„å®£ä¼ï¼‰ã‚’é™¤å¤–
    ng_words = [
        'ãã®ä»–',
        'ãŠã™ã™ã‚å‹•ç”»',
        'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²',
        'é–¢é€£å‹•ç”»',
        'å‹•ç”»ä¸€è¦§',
        'SNS',
        'Twitter',
        'Instagram',
        'LINE',
        'ã‚¨ãƒƒã‚»ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ',
        'ç°¡æ˜“ç‰ˆ',
        'Audible',
        'Kindle',
        'æœ¬ã‚’è´ã',
        'åˆ†è§£èª¬',
        'è¦ç´„',
        'è§£èª¬',
        'ã¾ã¨ã‚',
        'ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ',
        'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³',
        'ç„¡æ–™',
        'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«',
        'ãŠå•ã„åˆã‚ã›',
        'ãƒ¡ãƒ³ãƒãƒ¼ã‚·ãƒƒãƒ—',
        'ã‚µãƒ–ãƒãƒ£ãƒ³ãƒãƒ«',
        # ä¸ƒç€¬ã‚¢ãƒªãƒ¼ã‚µé–¢é€£ã®å®£ä¼ã‚’é™¤å¤–
        'ä¸ƒç€¬åˆ¶ä½œ',
        'å•†å“ç´¹ä»‹',
        'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚«ãƒ¼ãƒ‰',
        'Success Book',
        'Your Success',
        'è³¼å…¥ãƒšãƒ¼ã‚¸',
        'ç‰¹å…¸',
        'ãŠã™ã™ã‚é †ã§ã¯ãªã„',
        'æ¦‚è¦æ¬„',
        'ãƒ‡ã‚¸ã‚¿ãƒ«ç‰ˆ',
        'å†Šå­ç‰ˆ',
        # YouTuberè‡ªè‘—ã®å®£ä¼ã‚’é™¤å¤–
        'OUTPUTèª­æ›¸è¡“',
        'é€±åˆŠSPA',
        'äººç”Ÿã‚’å¤‰ãˆã‚‹ å“²å­¦è€…ã®è¨€è‘‰366',
        'ç¬é–“è‹±ä½œæ–‡',
        # åŒ–ç²§å“ãƒ»ç¾å®¹ç”¨å“ã‚’é™¤å¤–
        'Etude House BB cream',
        'Biooil',
        'Biore Sunscreen',
        "Visse's stick concealer",
        'Blush, eyeshadow pallet',
        "Visse's powder foundation",
        'Eyeblow powder',
        "Eyebrow's mascara",
        'lip balm',
        "Visse's powder blush",
        'IVY lip stick PK-300',
        'Hair Spray',
        'Panasonic 32mm hair iron ionity',
        'Find out more about Star Wars',
        # ãã®ä»–å•†å“ã‚’é™¤å¤–
        'ãƒ•ã‚£ãƒ¼ãƒãƒ¼ãƒ’ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼',
        'ã‚³ãƒ¼ãƒ’ãƒ¼è±†ï¼ˆæˆåŸçŸ³äº•ã®ï¼‰',
        'ã‚·ãƒªã‚«æ°´ãƒ¬ã‚¸ãƒ¼ãƒŠ',
        'ãºã‚“ã¦ã‚‹',
        'ãƒ¨ã‚¬ãƒãƒƒãƒˆ',
        'ã‚·ãƒªã‚«',
        'VOX',
        'ã‚³ãƒ¼ãƒ’ãƒ¼ãƒ¡ãƒ¼ã‚«ãƒ¼',
        # é£Ÿå“ãƒ»æ—¥ç”¨å“ã‚’é™¤å¤–
        'ç™½ä¸¸ã¨ã‚“ã“ã¤è±†è…ã‚¹ãƒ¼ãƒ—',
        'ãƒ™ã‚¸ç„ç±³ãƒ©ãƒ¼ãƒ¡ãƒ³ï¼ˆã—ãŠï¼‰',
        'å›½ç”£ã“ã‚“ã«ã‚ƒããƒ©ãƒ¼ãƒ¡ãƒ³ï¼ˆ25ãƒ‘ãƒƒã‚¯å…¥ã‚Šï¼‰',
        'å¤§è±†éººï¼ˆç´°éºº3äººå‰ï¼‰',
        'æ²³æ‘é€šå¤«ã®å¤§è‡ªç„¶ãƒ©ãƒ¼ãƒ¡ãƒ³',
        'ãƒœã‚¿ãƒ‹ã‚«ãƒ«é™¤èŒæ¶ˆè‡­ãƒŸã‚¹ãƒˆ(ç„¡é¦™æ–™)',
        'ã‚¤ã‚ªãƒ³æ¶ˆè‡­ãƒ—ãƒ©ã‚¹ã€€ç‰¹å¤§ã‚µã‚¤ã‚º1.5kgï¼ˆç„¡é¦™æ–™ï¼‰',
        'ã‚†ãšæ²¹ã€€ç„¡æ·»åŠ ãƒ˜ã‚¢ã‚ªã‚¤ãƒ«60mL',
        'UVã‚¤ãƒ‡ã‚¢ãƒ—ãƒ­ãƒ†ã‚¯ã‚·ãƒ§ãƒ³ãƒˆãƒ¼ãƒ³ã‚¢ãƒƒãƒ—',
        'ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒ»ãƒ•ã‚§ã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ»ã‚«ãƒ•ã‚§ã‚¤ãƒ³ãƒ¬ã‚¹ãƒ»ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ãƒˆã‚³ãƒ¼ãƒ’ãƒ¼',
        'æœ‰æ©Ÿã‚¯ã‚³ã®å®Ÿ100g',
        'ã‚ãã‚Šã‚ºãƒ ãƒ»è’¸æ°—ã§ãƒ›ãƒƒãƒˆã‚¢ã‚¤ãƒã‚¹ã‚¯',
        'ã‚¹ãƒã‚¤ãƒ«ã‚¶ãƒ¡ãƒ‡ã‚£ã‚«ãƒ«Aãƒ»DX',
        'ã¶ã©ã†å±±æ¤’çŸ³è‡¼æŒ½ãç²‰æœ«12g',
        'ã¶ã©ã†å±±æ¤’ç²’15g',
        'å›½ç”£ãƒ—ãƒ¼ã‚¢ãƒ«èŒ¶ãƒ»ãƒ†ã‚£ãƒ¼ãƒãƒƒã‚°2gÃ—25åŒ…',
        'ã‚¢ãƒ³ãƒ‰ã‚°ãƒƒãƒ‰ãƒŠã‚¤ãƒˆè–¬ç”¨å…¥æµ´å‰¤',
        'ãƒ‡ã‚ªãƒ‰ãƒ©ãƒ³ãƒˆã‚½ãƒ¼ãƒ—80g',
        'ãƒ“ãƒ¥ãƒ¼ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ»UVãƒ—ãƒ­ãƒ†ã‚¯ãƒˆãƒ»ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ',
        'ç„¡æ·»åŠ ãƒ»ç„¼è‚‰ã®ãŸã‚Œ',
        'ã‚¦ã‚£ãƒ«ã‚­ãƒ³ã‚½ãƒ³ã€Œã‚¿ãƒ³ã‚µãƒ³ãƒ©ãƒ™ãƒ«ãƒ¬ã‚¹ãƒœãƒˆãƒ«500mlÃ—24æœ¬ã€',
        'ã‚«ãƒ•ã‚§ã‚¤ãƒ³ãƒ¬ã‚¹æœ‰æ©Ÿã»ã†ã˜èŒ¶15P',
        'ã‚ªãƒ«ãƒŠ ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ ã‚·ãƒ£ãƒ³ãƒ—ãƒ¼',
        'ãƒ‘ã‚­ã‚¹ã‚¿ãƒ³ç”£ãƒ’ãƒãƒ©ãƒ¤å²©å¡©ã€Œãƒ€ãƒ¼ã‚¯ãƒ”ãƒ³ã‚¯ã€',
        'ç´”ã‚Šã‚“ã”é…¢',
        'ã‚¯ã‚¤ãƒƒã‚¯ãƒ«ãƒ¯ã‚¤ãƒ‘ãƒ¼',
        'ãƒ“ã‚ªã‚¹ãƒªãƒ¼HiéŒ ',
        'ãƒŸãƒ¤ãƒªã‚µãƒ³éŒ ',
        'ã¹ã£ã´ã‚“ã¯ã¨ã‚€ãã‚¹ãƒŠãƒƒã‚¯ã‚¿ã‚¤ãƒ—',
        'å›½ç”£ã‚ˆã‚‚ãç²‰æœ«',
        'ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒ»ã‚¨ã‚­ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚¸ãƒ³ãƒ»ã‚ªãƒªãƒ¼ãƒ–ã‚ªã‚¤ãƒ«',
        'ã‚¶ãƒ—ãƒ­ã‚°ãƒ©ã‚¹ãƒ•ã‚§ãƒƒãƒ‰ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³1Kgãƒ—ãƒ¬ãƒ¼ãƒ³',
        'ãƒã‚¦ãƒ³ãƒˆãƒãƒ¼ã‚²ãƒ³ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒ•ã‚§ã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ãƒˆã‚³ãƒ¼ãƒ’ãƒ¼',
        'å›½ç”£ã²ãã‚ã‚Šç´è±†',
        'æœ‰æ©Ÿæ ½åŸ¹ãƒ»å›½ç”£æ‰‹ä½œã‚Šã‚°ã‚¡ãƒèŒ¶',
        'ä½åˆ†å­ã‚³ãƒ©ãƒ¼ã‚²ãƒ³',
        'ã‚¼ãƒ©ãƒãƒ³',
        'Lamicallï¼ˆãƒ©ãƒŸã‚³ãƒ¼ãƒ«ï¼‰ã®ã‚¹ãƒãƒ›ãƒ›ãƒ«ãƒ€ãƒ¼',
        'Tapo ã‚¹ãƒãƒ¼ãƒˆãƒ—ãƒ©ã‚°',
        'è±¡å°ã®ç‚èˆç‚Šã ï¼®ï¼¸â€ï¼¡ï¼¡ï¼‘ï¼',
        'ã‚°ãƒ¬ã‚´ãƒªãƒ¼ ã‚¹ã‚±ãƒƒãƒ Sketch 30',
        'Apple AirPods Pro 3',
        '[Horus X] ã‚²ãƒ¼ãƒŸãƒ³ã‚°ã‚°ãƒ©ã‚¹',
        'ãƒ–ãƒ«ãƒ¼ãƒ©ã‚¤ãƒˆ 99.9% ã‚«ãƒƒãƒˆ pcãƒ¡ã‚¬ãƒ',
        'ERGOTRON ã‚¨ãƒ«ã‚´ãƒˆãƒ­ãƒ³ ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼',
        'ãƒ•ã‚§ã‚¤ã‚¯è¦³è‘‰æ¤ç‰©ä¼¼ãŸã‚„ã¤',
        'Mainichi Eikaiwa',
        'ã¤ã°ã‚ã®ãƒãƒ¼ãƒˆ',
        'ã‚¶ãƒ¬ãƒˆãƒªãƒƒã‚¯',
        'å­£ç¯€ã®çˆç²ï¼ˆæ˜¥ãŒè¦‹ã¤ã‹ã‚‰ãªã„..)',
        'é‹(ãƒ†ã‚£â—ãƒ¼ãƒ«ã‚ˆã‚Šå®‰ã„ã—å¯æ„›ã„ï¼‰',
        'ãƒãƒ§ãƒ¼ãƒ¤ã®æ¢…é…’',
    ]

    for ng in ng_words:
        if ng in title:
            return False

    # ã€Œæœ¬ã€ã ã‘ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’é™¤å¤–
    if title in ['æœ¬', 'æ›¸ç±', 'å›³æ›¸', 'book', 'books']:
        return False

    # ã€Œã€‡æœ¬ã‚»ãƒƒãƒˆã€ã®ã‚ˆã†ãªå•†å“è¡¨è¨˜ã‚’é™¤å¤–
    if re.search(r'\d+(æœ¬|å†Š)ã‚»ãƒƒãƒˆ', title):
        return False

    # å®¹é‡è¡¨è¨˜ã‚’å«ã‚€å•†å“ã‚’é™¤å¤–ï¼ˆä¾‹: 145gã€250mlã€1.5kgï¼‰
    if re.search(r'\d+(\.\d+)?\s*(g|kg|ml|mL|L)\b', title, re.IGNORECASE):
        return False

    # ä¾¡æ ¼/å®¹é‡è¡¨è¨˜ã‚’é™¤å¤–ï¼ˆä¾‹: å††/gï¼‰
    if re.search(r'å††/(g|kg|ml|mL|L)\b', title, re.IGNORECASE):
        return False

    # è‘—è€…åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–: ã€Œã€‡ã€‡(è‘—)ã€ã€Œã€‡ã€‡ï¼ˆè‘—ï¼‰ã€ã€Œã€‡ã€‡ã•ã¾ã€ã®ã¿ã®è¡Œ
    if re.search(r'[ï¼ˆ(]è‘—[ï¼‰)]', title) and 'ã€' not in title and 'ã€Œ' not in title:
        return False
    if re.match(r'^[\w\sãƒ»ã€€]+ã•ã¾[\sã€€]*$', title):
        return False

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã‚’é™¤å¤–: ã€Œè‘—è€…ï¼šã€‡ã€‡ã€ã€Œå‡ºç‰ˆç¤¾ï¼šã€‡ã€‡ã€ã€Œå‡ºç‰ˆç¤¾ã€€ã€‡ã€‡ã€ã€Œè‘—ã€€è€…ã€ã€Œç·¨é›†ã€€ã€‡ã€‡ã€
    if re.match(r'^(è‘—[\sã€€]*è€…|å‡ºç‰ˆç¤¾|å‡ºç‰ˆ|ç™ºè¡Œ|æ›¸ç±|ç·¨é›†|ç¿»è¨³|ç›£ä¿®)[\s\u200f\u200e]*[ï¼š:.ã€€\s]', title):
        return False
    # ã€Œã€‡ã€‡ (ç·¨é›†)ã€ã€Œã€‡ã€‡ (ç·¨è‘—)ã€ã€Œã€‡ã€‡ (ç¿»è¨³)ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–
    if re.search(r'[ï¼ˆ(](ç·¨é›†|ç·¨è‘—|ç›£ä¿®|ç¿»è¨³)[ï¼‰)][\s]*$', title):
        return False

    # è‘—ä½œæ¨©ãƒ»è¨±è«¾è¡¨è¨˜ã‚’é™¤å¤–
    if re.search(r'(è¨±è«¾ã‚’å¾—ã¦|é…ä¿¡ã—ã¦ãŠã‚Šã¾ã™|æä¾›ã§ãŠé€ã‚Š|ã‚¿ã‚¤ã‚¢ãƒƒãƒ—)', title):
        return False
    # èª¬æ˜æ–‡ãƒ»æ¡ˆå†…æ–‡ã‚’é™¤å¤–
    if title.startswith('æœ¬å‹•ç”»ã¯'):
        return False
    if re.search(r'(ã‚¢ãƒã‚¾ãƒ³ã§è³¼å…¥|Amazonã§è³¼å…¥|è³¼å…¥ã§ãã¾ã™|è³¼å…¥ã¯ã“ã¡ã‚‰)', title):
        return False


    # URLã£ã½ã„ã‚‚ã®ã‚’é™¤å¤–
    if 'http' in title.lower() or '.com' in title.lower():
        return False

    # å…¨ã¦è¨˜å·ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’é™¤å¤–
    if all(not c.isalnum() for c in title):
        return False

    # YouTuberåãŒå…¥ã£ã¦ã„ã‚‹ã‚‚ã®ã‚’é™¤å¤–ï¼ˆè‡ªè‘—å®£ä¼ã®å¯èƒ½æ€§ï¼‰
    youtuber_names = ['ã‚¢ãƒã‚¿ãƒ­ãƒ¼', 'ã‚µãƒ©ã‚¿ãƒ¡', 'æœ¬è¦ç´„ãƒãƒ£ãƒ³ãƒãƒ«', 'å­¦è­˜ã‚µãƒ­ãƒ³', 'ãƒ•ã‚§ãƒ«ãƒŸ', 'ä¸‰å®…', 'ä¸ƒç€¬', 'ã‚¢ãƒªãƒ¼ã‚µ']
    for name in youtuber_names:
        if name in title:
            return False

    return True


def normalize_title_key(title):
    """è¡¨è¨˜æºã‚Œçµ±ä¸€ç”¨ã®æ­£è¦åŒ–ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
    t = title
    t = re.sub(r'[ã€ã€ã€Œã€]', '', t)
    t = re.sub(r'[ï¼ˆ(](å˜è¡Œæœ¬|æ–‡åº«|æ–°æ›¸|ãƒãƒ¼ãƒ‰ã‚«ãƒãƒ¼|Kindleç‰ˆ)[ï¼‰)]', '', t)
    t = re.sub(r'^(æ”¹è¨‚ç‰ˆ|æ–°ç‰ˆ|æ–°è£…ç‰ˆ|å¢—è£œç‰ˆ|æ±ºå®šç‰ˆ|å®Œå…¨ç‰ˆ)\s*', '', t)
    t = re.sub(r'(æ”¹è¨‚ç‰ˆã§ã™|æ”¹è¨‚ç‰ˆ)$', '', t)
    t = re.sub(r'[\sã€€ã€,ï¼š:]+', '', t)
    t = t.lower()
    return t


def merge_similar_books(all_books):
    """çŸ­ã„ã‚­ãƒ¼ãŒé•·ã„ã‚­ãƒ¼ã®å…ˆé ­ã«å«ã¾ã‚Œã‚‹å ´åˆã€åŒä¸€æ›¸ç±ã¨ã—ã¦çµ±åˆ"""
    keys = sorted(all_books.keys(), key=len)
    merge_map = {}  # short_key -> long_key (çµ±åˆå…ˆ)
    for i, short_key in enumerate(keys):
        if short_key in merge_map or len(short_key) < 5:
            continue
        for long_key in keys[i+1:]:
            if long_key in merge_map:
                continue
            if long_key.startswith(short_key):
                merge_map[short_key] = long_key
                break  # æœ€çŸ­ã®çµ±åˆå…ˆã«çµ±åˆ

    for src_key, dst_key in merge_map.items():
        src = all_books.pop(src_key, None)
        if not src or dst_key not in all_books:
            continue
        dst = all_books[dst_key]
        dst["count"] += src["count"]
        dst["total_views"] += src["total_views"]
        dst["total_likes"] += src["total_likes"]
        dst["videos"].extend(src["videos"])
        dst["_title_variants"].extend(src.get("_title_variants", [src["title"]]))
        if not dst.get("author") and src.get("author"):
            dst["author"] = src["author"]
        if not dst.get("publisher") and src.get("publisher"):
            dst["publisher"] = src["publisher"]


def choose_canonical_title(titles):
    """è¤‡æ•°ã®è¡¨è¨˜æºã‚Œã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æœ€ã‚‚æ­£å¼ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’é¸æŠ"""
    cleaned = [re.sub(r'\s*[ï¼ˆ(](å˜è¡Œæœ¬|æ–‡åº«|æ–°æ›¸|ãƒãƒ¼ãƒ‰ã‚«ãƒãƒ¼|Kindleç‰ˆ)[ï¼‰)]', '', t) for t in titles]
    with_subtitle = [t for t in cleaned if 'ï¼š' in t or ':' in t or 'â€•' in t or 'â€”' in t]
    candidates = with_subtitle if with_subtitle else cleaned
    return max(candidates, key=len)


def generate_amazon_search_url(book_title):
    """æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰Amazonæ¤œç´¢URLã‚’ç”Ÿæˆï¼ˆã‚¢ã‚½ã‚·ã‚¨ã‚¤ãƒˆã‚¿ã‚°ä»˜ãï¼‰"""
    query = urllib.parse.quote(book_title)
    return f"https://www.amazon.co.jp/s?k={query}&i=stripbooks&tag={AMAZON_TRACKING_ID}"


def generate_book_id(title):
    """æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ç”Ÿæˆ"""
    return hashlib.md5(title.encode()).hexdigest()[:12]


# =============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =============================================================================

def load_channels():
    with open(CHANNELS_FILE, "r", encoding="utf-8") as f:
        return json.load(f)["channels"]


def main():
    if not YOUTUBE_API_KEY:
        print("ERROR: YOUTUBE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    os.makedirs(DATA_DIR, exist_ok=True)
    channels = load_channels()
    all_books = {}

    for ch in channels:
        channel_name = ch["name"]
        channel_id = ch["channel_id"]
        print(f"\n=== {channel_name} (ID: {channel_id}) ===")

        videos = fetch_all_channel_videos(channel_id)

        for video in videos:
            summary = video.get("summary", "")
            video_title = video.get("title", "")
            book_info_list = extract_book_info_list(summary, video_title)

            if not book_info_list:
                continue

            for book_info in book_info_list:
                book_title = book_info.get("title")
                if not book_title:
                    continue

                # ã‚¿ã‚¤ãƒˆãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆè‘—è€…åãƒ»å‡ºç‰ˆç¤¾ã‚’åˆ†é›¢ï¼‰
                book_title = clean_book_title(book_title)
                book_info["title"] = book_title

                # ã‚¿ã‚¤ãƒˆãƒ«ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                if not is_valid_book_title(book_title):
                    continue

                # è‡ªè‘—å®£ä¼ã‚¹ã‚­ãƒƒãƒ—
                if book_info.get("_is_first") and len(book_info_list) > 1:
                    continue

                # Amazonãƒªãƒ³ã‚¯ã‹ã‚‰å–å¾—ã—ãŸå ´åˆã¯æ—¢ã«amazon_urlãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹
                amazon_url = book_info.get("amazon_url") or generate_amazon_search_url(book_title)

                # è¡¨è¨˜æºã‚Œçµ±ä¸€: æ­£è¦åŒ–ã‚­ãƒ¼ã§åŒä¸€æ›¸ç±ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                norm_key = normalize_title_key(book_title)

                if norm_key not in all_books:
                    all_books[norm_key] = {
                        "id": generate_book_id(norm_key),
                        "title": book_title,
                        "_title_variants": [book_title],
                        "author": book_info.get("author"),
                        "publisher": book_info.get("publisher"),
                        "amazon_url": amazon_url,
                        "count": 0,
                        "total_views": 0,
                        "total_likes": 0,
                        "videos": [],
                    }
                else:
                    # æ–°ã—ã„ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¨˜éŒ²
                    if book_title not in all_books[norm_key]["_title_variants"]:
                        all_books[norm_key]["_title_variants"].append(book_title)
                    # è‘—è€…ãƒ»å‡ºç‰ˆç¤¾ãŒæœªè¨­å®šãªã‚‰è£œå®Œ
                    if not all_books[norm_key]["author"] and book_info.get("author"):
                        all_books[norm_key]["author"] = book_info["author"]
                    if not all_books[norm_key]["publisher"] and book_info.get("publisher"):
                        all_books[norm_key]["publisher"] = book_info["publisher"]

                all_books[norm_key]["count"] += 1
                all_books[norm_key]["total_views"] += video.get("view_count", 0)
                all_books[norm_key]["total_likes"] += video.get("like_count", 0)
                all_books[norm_key]["videos"].append({
                    "video_id": video["video_id"],
                    "video_title": video["title"],
                    "channel": channel_name,
                    "link": video["link"],
                    "published": video["published"],
                    "view_count": video.get("view_count", 0),
                    "like_count": video.get("like_count", 0),
                })

    # --- è¡¨è¨˜æºã‚Œçµ±ä¸€ ---
    # 1. çŸ­ã„ã‚­ãƒ¼ãŒé•·ã„ã‚­ãƒ¼ã«å«ã¾ã‚Œã‚‹å ´åˆã‚’çµ±åˆ
    merge_similar_books(all_books)
    # 2. å„ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æ­£è¦ã‚¿ã‚¤ãƒˆãƒ«ã‚’é¸æŠ
    for book in all_books.values():
        variants = book.pop("_title_variants", [book["title"]])
        if len(variants) > 1:
            canonical = choose_canonical_title(variants)
            book["title"] = canonical
            book["amazon_url"] = generate_amazon_search_url(canonical)

    # --- çµæœè¡¨ç¤º ---
    books_list = list(all_books.values())
    print(f"\n=== æŠ½å‡ºçµæœ ===")
    print(f"æ›¸ç±æ•°: {len(books_list)}")

    # --- æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ã®ãƒãƒ¼ã‚¸ï¼ˆISBNç­‰ã‚’ä¿æŒï¼‰ ---
    books_file = os.path.join(DATA_DIR, "books.json")
    if os.path.exists(books_file):
        with open(books_file, "r", encoding="utf-8") as f:
            existing_books = json.load(f)
        # idã§ãƒãƒƒãƒ—åŒ–
        existing_map = {b["id"]: b for b in existing_books}
        # ã‚¿ã‚¤ãƒˆãƒ«æ­£è¦åŒ–ã‚­ãƒ¼ã§ã‚‚ãƒãƒƒãƒ—åŒ–ï¼ˆIDãŒå¤‰ã‚ã£ãŸå ´åˆã«å¯¾å¿œï¼‰
        existing_by_title = {normalize_title_key(b["title"]): b for b in existing_books}
        # æ–°ãƒ‡ãƒ¼ã‚¿ã«æ—¢å­˜ã®ISBN/ASIN/image_urlç­‰ã‚’ãƒãƒ¼ã‚¸
        for book in books_list:
            # IDã§ãƒãƒƒãƒã€ã¾ãŸã¯ã‚¿ã‚¤ãƒˆãƒ«æ­£è¦åŒ–ã‚­ãƒ¼ã§ãƒãƒƒãƒ
            existing = existing_map.get(book["id"])
            if not existing:
                norm_key = normalize_title_key(book["title"])
                existing = existing_by_title.get(norm_key)
            if existing:
                for key in ["isbn", "asin", "image_url", "publication_date", "openbd_title"]:
                    if existing.get(key) and not book.get(key):
                        book[key] = existing[key]
                # amazon_urlã¯ASINä»˜ãã®ã‚‚ã®ã‚’å„ªå…ˆ
                if existing.get("asin") and "/dp/" in existing.get("amazon_url", ""):
                    book["amazon_url"] = existing["amazon_url"]
                    book["asin"] = existing["asin"]

    # --- JSONç”Ÿæˆ ---

    # books.jsonï¼ˆç´¹ä»‹å›æ•°é †ï¼‰
    books_by_count = sorted(books_list, key=lambda x: x["count"], reverse=True)
    with open(books_file, "w", encoding="utf-8") as f:
        json.dump(books_by_count, f, ensure_ascii=False, indent=2)

    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”¨ã®è»½é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    def make_ranking_entry(book):
        return {
            "id": book["id"],
            "title": book["title"],
            "author": book.get("author"),
            "count": book["count"],
            "total_views": book["total_views"],
            "total_likes": book["total_likes"],
            "amazon_url": book["amazon_url"],
        }

    # rankings.jsonï¼ˆç´¹ä»‹å›æ•°é †ï¼‰
    rankings_count = [make_ranking_entry(b) for b in books_by_count]
    with open(os.path.join(DATA_DIR, "rankings.json"), "w", encoding="utf-8") as f:
        json.dump(rankings_count, f, ensure_ascii=False, indent=2)

    # rankings_views.jsonï¼ˆå†ç”Ÿå›æ•°åˆè¨ˆé †ï¼‰
    books_by_views = sorted(books_list, key=lambda x: x["total_views"], reverse=True)
    rankings_views = [make_ranking_entry(b) for b in books_by_views]
    with open(os.path.join(DATA_DIR, "rankings_views.json"), "w", encoding="utf-8") as f:
        json.dump(rankings_views, f, ensure_ascii=False, indent=2)

    # rankings_likes.jsonï¼ˆã„ã„ã­åˆè¨ˆé †ï¼‰
    books_by_likes = sorted(books_list, key=lambda x: x["total_likes"], reverse=True)
    rankings_likes = [make_ranking_entry(b) for b in books_by_likes]
    with open(os.path.join(DATA_DIR, "rankings_likes.json"), "w", encoding="utf-8") as f:
        json.dump(rankings_likes, f, ensure_ascii=False, indent=2)

    print(f"\n--- TOP20ï¼ˆç´¹ä»‹å›æ•°é †ï¼‰---")
    for i, book in enumerate(books_by_count[:20], 1):
        print(f"  {i}. ã€{book['title']}ã€ (ç´¹ä»‹{book['count']}å› / å†ç”Ÿ{book['total_views']:,} / ã„ã„ã­{book['total_likes']:,})")

    print(f"\n--- TOP10ï¼ˆå†ç”Ÿå›æ•°é †ï¼‰---")
    for i, book in enumerate(books_by_views[:10], 1):
        print(f"  {i}. ã€{book['title']}ã€ (å†ç”Ÿ{book['total_views']:,} / ç´¹ä»‹{book['count']}å›)")

    print(f"\n--- TOP10ï¼ˆã„ã„ã­é †ï¼‰---")
    for i, book in enumerate(books_by_likes[:10], 1):
        print(f"  {i}. ã€{book['title']}ã€ (ã„ã„ã­{book['total_likes']:,} / ç´¹ä»‹{book['count']}å›)")

    print(f"\nãƒ‡ãƒ¼ã‚¿ã‚’ {DATA_DIR} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()
