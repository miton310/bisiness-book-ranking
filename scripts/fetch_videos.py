#!/usr/bin/env python3
"""YouTube Data APIã§å…¨å‹•ç”»ã‚’å–å¾—ã—ã€æ›¸ç±æƒ…å ±ã‚’æŠ½å‡ºã—ã¦JSONã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
  python fetch_videos.py          # å·®åˆ†æ›´æ–°ï¼ˆå‰å›ä»¥é™ã®æ–°ã—ã„å‹•ç”»ã®ã¿ï¼‰
  python fetch_videos.py --full   # å…¨ä»¶å–å¾—ï¼ˆåˆå›å®Ÿè¡Œæ™‚ã‚„å®Œå…¨ãƒªã‚»ãƒƒãƒˆæ™‚ï¼‰
"""

import argparse
import hashlib
import json
import os
import re
import sys
import time
import urllib.parse
import urllib.request
from datetime import datetime

# Amazonãƒªãƒ³ã‚¯ã‹ã‚‰æ›¸ç±æƒ…å ±å–å¾—
from fetch_amazon_info import extract_books_from_amazon_links

DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
CHANNELS_FILE = os.path.join(DATA_DIR, "channels.json")
FETCH_STATE_FILE = os.path.join(DATA_DIR, "fetch_state.json")

YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY", "")
if not YOUTUBE_API_KEY:
    # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
    env_path = os.path.join(os.path.dirname(__file__), "..", ".env")
    if os.path.exists(env_path):
        with open(env_path) as f:
            for line in f:
                if line.startswith("YOUTUBE_API_KEY="):
                    YOUTUBE_API_KEY = line.strip().split("=", 1)[1]

AMAZON_ASSOCIATE_TAG = "miton31003"
AMAZON_TRACKING_ID = "business-book-ranking02-22"

YOUTUBE_API_BASE = "https://www.googleapis.com/youtube/v3"


# =============================================================================
# YouTube Data API
# =============================================================================

def api_get(endpoint, params):
    """YouTube Data API ã«GETãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    params["key"] = YOUTUBE_API_KEY
    url = f"{YOUTUBE_API_BASE}/{endpoint}?" + urllib.parse.urlencode(params)
    req = urllib.request.Request(url)
    with urllib.request.urlopen(req) as resp:
        return json.loads(resp.read().decode())


def get_uploads_playlist_id(channel_id):
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å†ç”Ÿãƒªã‚¹ãƒˆIDã‚’å–å¾—"""
    data = api_get("channels", {
        "part": "contentDetails",
        "id": channel_id,
    })
    items = data.get("items", [])
    if not items:
        return None
    return items[0]["contentDetails"]["relatedPlaylists"]["uploads"]


def get_all_video_ids(playlist_id, since=None):
    """å†ç”Ÿãƒªã‚¹ãƒˆã‹ã‚‰å‹•ç”»IDã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰

    Args:
        playlist_id: YouTubeã®ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆID
        since: ã“ã®æ—¥æ™‚ä»¥é™ã®å‹•ç”»ã®ã¿å–å¾—ï¼ˆISO 8601å½¢å¼ï¼‰ã€‚Noneãªã‚‰å…¨ä»¶å–å¾—ã€‚
    """
    video_ids = []
    page_token = None
    stop_fetching = False

    while not stop_fetching:
        params = {
            "part": "snippet",
            "playlistId": playlist_id,
            "maxResults": 50,
        }
        if page_token:
            params["pageToken"] = page_token
        data = api_get("playlistItems", params)

        for item in data.get("items", []):
            published = item["snippet"].get("publishedAt", "")
            vid = item["snippet"]["resourceId"]["videoId"]

            # å·®åˆ†æ›´æ–°: sinceã‚ˆã‚Šå¤ã„å‹•ç”»ãŒå‡ºãŸã‚‰åœæ­¢
            if since and published and published <= since:
                stop_fetching = True
                break

            video_ids.append(vid)

        page_token = data.get("nextPageToken")
        if not page_token:
            break
        time.sleep(0.1)

    return video_ids


def parse_iso8601_duration(duration_str):
    """ISO 8601ã®durationæ–‡å­—åˆ—ã‚’ç§’æ•°ã«å¤‰æ›ï¼ˆä¾‹: PT1M30S â†’ 90ï¼‰"""
    match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str or '')
    if not match:
        return 0
    hours = int(match.group(1) or 0)
    minutes = int(match.group(2) or 0)
    seconds = int(match.group(3) or 0)
    return hours * 3600 + minutes * 60 + seconds


def get_video_details(video_ids):
    """å‹•ç”»IDãƒªã‚¹ãƒˆã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆ50ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†ï¼‰
    60ç§’ä»¥ä¸‹ã®ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»ã¯é™¤å¤–ã™ã‚‹"""
    videos = []
    shorts_count = 0
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        data = api_get("videos", {
            "part": "snippet,statistics,contentDetails",
            "id": ",".join(batch),
        })
        for item in data.get("items", []):
            # ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»ã‚’é™¤å¤–ï¼ˆ60ç§’ä»¥ä¸‹ï¼‰
            duration_str = item.get("contentDetails", {}).get("duration", "")
            duration_sec = parse_iso8601_duration(duration_str)
            if duration_sec <= 60:
                shorts_count += 1
                continue
            snippet = item["snippet"]
            stats = item.get("statistics", {})
            videos.append({
                "video_id": item["id"],
                "title": snippet["title"],
                "published": snippet["publishedAt"],
                "link": f"https://www.youtube.com/watch?v={item['id']}",
                "summary": snippet.get("description", ""),
                "channel_title": snippet.get("channelTitle", ""),
                "view_count": int(stats.get("viewCount", 0)),
                "like_count": int(stats.get("likeCount", 0)),
            })
        time.sleep(0.1)
    if shorts_count > 0:
        print(f"  ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»ã‚’é™¤å¤–: {shorts_count}ä»¶")
    return videos


def fetch_all_channel_videos(channel_id, since=None):
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®å‹•ç”»ã‚’å–å¾—

    Args:
        channel_id: YouTubeãƒãƒ£ãƒ³ãƒãƒ«ID
        since: ã“ã®æ—¥æ™‚ä»¥é™ã®å‹•ç”»ã®ã¿å–å¾—ã€‚Noneãªã‚‰å…¨ä»¶å–å¾—ã€‚
    """
    playlist_id = get_uploads_playlist_id(channel_id)
    if not playlist_id:
        print(f"  [ERROR] ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å†ç”Ÿãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []
    video_ids = get_all_video_ids(playlist_id, since=since)
    if since:
        print(f"  æ–°è¦å‹•ç”»IDå–å¾—: {len(video_ids)}ä»¶ (since: {since[:10]})")
    else:
        print(f"  å‹•ç”»IDå–å¾—: {len(video_ids)}ä»¶")
    if not video_ids:
        return []
    videos = get_video_details(video_ids)
    print(f"  å‹•ç”»è©³ç´°å–å¾—: {len(videos)}ä»¶")
    return videos


def load_fetch_state():
    """å‰å›ã®å–å¾—çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã‚€"""
    if os.path.exists(FETCH_STATE_FILE):
        with open(FETCH_STATE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_fetch_state(state):
    """å–å¾—çŠ¶æ…‹ã‚’ä¿å­˜"""
    with open(FETCH_STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


# =============================================================================
# æ›¸ç±æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
# =============================================================================

def extract_book_info_list(summary, video_title=None):
    """æ¦‚è¦æ¬„ãƒ»å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’æŠ½å‡º"""
    results = []

    # ãƒ‘ã‚¿ãƒ¼ãƒ³0: å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æŠ½å‡ºã€Œã€è¦ç´„ã€‘ã‚¿ã‚¤ãƒˆãƒ«ã€è‘—è€…ã€‘ã€ï¼ˆãƒ•ã‚§ãƒ«ãƒŸæ¼«ç”»å¤§å­¦ç­‰ï¼‰
    if video_title:
        m = re.match(r'ã€(?:è¦ç´„|æ¼«ç”»)ã€‘(.+?)ã€(.+?)ã€‘', video_title)
        if m:
            book_title = m.group(1).strip()
            author = m.group(2).strip()
            results.append({
                "title": book_title,
                "author": author,
                "publisher": None,
            })
            return results

    # TODO: Amazonãƒªãƒ³ã‚¯ã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’å–å¾—ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼‰
    # amazon_urls = re.findall(r'https?://amzn\.to/[A-Za-z0-9]+', summary)
    # if amazon_urls:
    #     amazon_books = extract_books_from_amazon_links(amazon_urls, max_books=5, context=summary)
    #     for book in amazon_books:
    #         results.append({
    #             "title": book["title"],
    #             "author": None,
    #             "publisher": None,
    #             "amazon_url": book["amazon_url"],
    #         })
    #     if results:
    #         return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æœ¬è¦ç´„ãƒãƒ£ãƒ³ãƒãƒ« / ã‚µãƒ©ã‚¿ãƒ¡ã•ã‚“ã€Œã‚¿ã‚¤ãƒˆãƒ«ï¼šã€ã€Œè‘—è€…ï¼šã€ã€Œå‡ºç‰ˆç¤¾ï¼šã€
    title_match = re.search(r'ã‚¿ã‚¤ãƒˆãƒ«[ï¼š:](.+)', summary)
    if title_match:
        info = {
            "title": title_match.group(1).strip(),
            "author": None,
            "publisher": None,
        }
        author_match = re.search(r'è‘—è€…[ï¼š:](.+)', summary)
        if author_match:
            info["author"] = author_match.group(1).strip()
        publisher_match = re.search(r'å‡ºç‰ˆç¤¾[ï¼š:](.+)', summary)
        if publisher_match:
            info["publisher"] = publisher_match.group(1).strip()
        results.append(info)
        return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ•ã‚§ãƒ«ãƒŸæ¼«ç”»å¤§å­¦ã€Œå‚è€ƒï¼šæ›¸å è‘—è€…å ã•ã¾ã€
    # ã€Œå‚è€ƒæ–‡çŒ®ï¼šã€ã‚‚å¯¾å¿œ
    ref_match = re.search(r'å‚è€ƒ(?:æ–‡çŒ®)?[ï¼š:](.+?)(?:\s+ã•ã¾|\s*$)', summary, re.MULTILINE)
    if ref_match:
        title_text = ref_match.group(1).strip()
        # è‘—è€…åã ã‘ã®è¡Œã‚’é™¤å¤–ï¼ˆã€Œã•ã¾ã€ã§çµ‚ã‚ã‚‹äººåã®ã¿ã€æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ãªã—ï¼‰
        if not re.match(r'^[\w\sãƒ»ã€€]+ã•ã¾', title_text) and title_text:
            results.append({
                "title": title_text,
                "author": None,
                "publisher": None,
            })
            return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: å­¦è­˜ã‚µãƒ­ãƒ³ã€Œã€amazonãƒªãƒ³ã‚¯ã€‘\nã€æ›¸åã€è‘—è€… / å‡ºç‰ˆç¤¾ã€
    if "ã€amazonãƒªãƒ³ã‚¯ã€‘" in summary:
        gakushiki_match = re.search(r'ã€(.+?)ã€(.+?)(?:\s*/\s*(.+))?$', summary, re.MULTILINE)
        if gakushiki_match:
            info = {
                "title": gakushiki_match.group(1).strip(),
                "author": None,
                "publisher": None,
            }
            if gakushiki_match.group(2):
                info["author"] = gakushiki_match.group(2).strip()
            if gakushiki_match.group(3):
                info["publisher"] = gakushiki_match.group(3).strip()
            results.append(info)
            return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ã‚µãƒ ã®æœ¬è§£èª¬chã€Œã€ä»Šå›ã®å‚è€ƒæ›¸ç±ğŸ“šã€‘ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    sam_section = re.search(
        r'ã€ä»Šå›ã®å‚è€ƒæ›¸ç±.*?ã€‘\s*\n(.*?)(?=ã€|$)', summary, re.DOTALL
    )
    if sam_section:
        section_text = sam_section.group(1).strip()
        lines = section_text.split('\n')
        title_line = None
        author_line = None
        for line in lines:
            line = line.strip()
            if not line or line.startswith('http'):
                continue
            # è‘—è€…è¡Œã‚’åˆ¤å®š: ã€Œã€œ(è‘—)ã€ã€Œã€œï¼ˆè‘—ï¼‰ã€ã‚’å«ã‚€è¡Œ
            if re.search(r'[ï¼ˆ(]è‘—[ï¼‰)]', line):
                author_line = line
            elif not title_line:
                # æœ€åˆã®éè‘—è€…è¡Œã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦å–å¾—
                title_line = re.sub(r'\s*(Kindleç‰ˆ|å˜è¡Œæœ¬|æ–‡åº«|æ–°æ›¸|ãƒãƒ¼ãƒ‰ã‚«ãƒãƒ¼)\s*$', '', line).strip()
                # å…ˆé ­ã®ã€Œãƒ»ã€ã‚’é™¤å»
                title_line = re.sub(r'^[ãƒ»ï½¥]', '', title_line).strip()
        if title_line:
            info = {"title": title_line, "author": None, "publisher": None}
            if author_line:
                author_match = re.match(r'(.+?)\s*[ï¼ˆ(]è‘—[ï¼‰)]', author_line)
                if author_match:
                    info["author"] = author_match.group(1).strip()
                pub_match = re.search(r'([^\s]+?)[ï¼ˆ(]ç·¨é›†[ï¼‰)]', author_line)
                if pub_match:
                    info["publisher"] = pub_match.group(1).strip()
            results.append(info)
            return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³5: PIVOTç³»ã€Œï¼œå‚è€ƒæ›¸ç±ï¼ã€ã€Œâ–¼å‚è€ƒæ›¸ç±ã€ã€Œâ–¼é–¢é€£æ›¸ç±ã€ã€Œâ–¼æœ¬æ˜ åƒã§ç´¹ä»‹ã—ãŸæ›¸ç±ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    pivot_section = re.search(
        r'(?:[ï¼œ<]å‚è€ƒæ›¸ç±[ï¼>]|â–¼å‚è€ƒæ›¸ç±|â–¼é–¢é€£æ›¸ç±|â–¼æœ¬æ˜ åƒã§ç´¹ä»‹ã—ãŸæ›¸ç±)\s*\n(.*?)(?=\n[ï¼œ<]|\nâ–¼[^å‚é–¢æœ¬]|\n[â– â—]|\nâ€»|\n\n\n|$)', summary, re.DOTALL
    )
    if pivot_section:
        section_text = pivot_section.group(1).strip()
        lines = section_text.split('\n')
        for line in lines:
            line = line.strip()
            if not line or line.startswith('http') or line.startswith('â€»'):
                continue

            title = None
            author = None

            # ãƒ‘ã‚¿ãƒ¼ãƒ³A: ã€ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚’å„ªå…ˆï¼ˆå†…éƒ¨ã«ã€Œã€ãŒå«ã¾ã‚Œã¦ã‚‚OKï¼‰
            book_match = re.search(r'ã€(.+?)ã€', line)
            if book_match:
                title = book_match.group(1).strip()
                before = line[:book_match.start()].strip()
                if before:
                    author = before
                after = line[book_match.end():].strip()
                if not author and after:
                    a_match = re.match(r'(.+?)\s*[ï¼ˆ(]è‘—[ï¼‰)]', after)
                    if a_match:
                        author = a_match.group(1).strip()

            # ãƒ‘ã‚¿ãƒ¼ãƒ³B: ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ï¼‹å¾Œç¶šãƒ†ã‚­ã‚¹ãƒˆã‚‚å«ã‚ã‚‹
            if not title:
                book_match = re.search(r'ã€Œ(.+?)ã€(.+?)(?=[ï¼ˆ(]|https?://|\s*$)', line)
                if book_match:
                    # ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ã®å¾Œã‚ã‚‚ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸€éƒ¨ã¨ã—ã¦çµåˆ
                    title = book_match.group(1).strip() + book_match.group(2).strip()
                    # æœ«å°¾ã®æ‹¬å¼§å†…ï¼ˆå‡ºç‰ˆç¤¾ç­‰ï¼‰ã‚’é™¤å»
                    title = re.sub(r'[ï¼ˆ(][^ï¼‰)]+[ï¼‰)]$', '', title).strip()

            if not title:
                continue

            results.append({
                "title": title,
                "author": author,
                "publisher": None,
            })
        # PIVOTã®å‚è€ƒæ›¸ç±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹å ´åˆã¯çµæœã«é–¢ã‚ã‚‰ãšã“ã“ã§è¿”ã™
        # ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³6ã®amzn.toæ±ç”¨æŠ½å‡ºã«è½ã¡ãªã„ã‚ˆã†ã«ã™ã‚‹ï¼‰
        return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³5.5: flierã€Œâ–¼ç´¹ä»‹ã—ãŸä½œå“ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    # å½¢å¼: â–¼ç´¹ä»‹ã—ãŸä½œå“
    #       è‘—è€…ã€ã‚¿ã‚¤ãƒˆãƒ«ã€ï¼ˆå‡ºç‰ˆç¤¾ï¼‰
    #       https://amzn.to/xxx
    # è¤‡æ•°ã®å ´åˆ: â‘ è‘—è€…ã€ã‚¿ã‚¤ãƒˆãƒ«ã€ï¼ˆå‡ºç‰ˆç¤¾ï¼‰
    flier_section = re.search(
        r'â–¼ç´¹ä»‹ã—ãŸä½œå“\s*\n(.*?)(?=\nâ–¼[^ç´¹]|\nâ€»ä¸Šè¨˜ãƒªãƒ³ã‚¯|$)', summary, re.DOTALL
    )

    # ãƒ‘ã‚¿ãƒ¼ãƒ³5.6: TBS CROSS DIGã€Œâ—†æ›¸ç±ç´¹ä»‹â—†ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    # å½¢å¼: â—†æ›¸ç±ç´¹ä»‹â—†
    #       â–¼ã€ã‚¿ã‚¤ãƒˆãƒ«ã€
    #       è‘—è€…
    #       å‡ºç‰ˆç¤¾
    #       https://amzn.to/xxx
    tbs_section = re.search(
        r'â—†æ›¸ç±ç´¹ä»‹â—†\s*\n(.*?)(?=\nâ—†|$)', summary, re.DOTALL
    )
    if tbs_section:
        section_text = tbs_section.group(1).strip()
        lines = section_text.split('\n')
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            # â–¼ã€ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚’æ¢ã™
            title_match = re.match(r'â–¼ã€(.+?)ã€', line)
            if title_match:
                title = title_match.group(1).strip()
                author = None
                publisher = None
                # æ¬¡ã®è¡Œã§è‘—è€…ã€ãã®æ¬¡ã§å‡ºç‰ˆç¤¾ã‚’å–å¾—
                if i + 1 < len(lines) and not lines[i+1].strip().startswith('http'):
                    author = lines[i+1].strip()
                if i + 2 < len(lines) and not lines[i+2].strip().startswith('http'):
                    publisher = lines[i+2].strip()
                results.append({
                    "title": title,
                    "author": author,
                    "publisher": publisher,
                })
            i += 1
        if results:
            return results
    if flier_section:
        section_text = flier_section.group(1).strip()
        lines = section_text.split('\n')
        for line in lines:
            line = line.strip()
            if not line or line.startswith('http') or line.startswith('â€»'):
                continue
            # â‘ â‘¡ç­‰ã®ç•ªå·ã‚’é™¤å»
            line = re.sub(r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]\s*', '', line)
            # è‘—è€…ã€ã‚¿ã‚¤ãƒˆãƒ«ã€ï¼ˆå‡ºç‰ˆç¤¾ï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³
            match = re.match(r'(.+?)ã€(.+?)ã€(?:ï¼ˆ(.+?)ï¼‰)?', line)
            if match:
                author = match.group(1).strip() if match.group(1) else None
                title = match.group(2).strip()
                publisher = match.group(3).strip() if match.group(3) else None
                results.append({
                    "title": title,
                    "author": author,
                    "publisher": publisher,
                })
        if results:
            return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³6: ä¸ƒç€¬ã‚¢ãƒªãƒ¼ã‚µ â€” amzn.toãƒªãƒ³ã‚¯ã‹ã‚‰æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
    # å½¢å¼A: ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€€https://amzn.to/xxxã€(åŒä¸€è¡Œ)
    # å½¢å¼B: ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€+ æ¬¡è¡Œã€Œhttps://amzn.to/xxxã€(åˆ¥è¡Œ)
    amazon_lines = re.findall(r'https?://amzn\.to/[A-Za-z0-9]+', summary)
    if amazon_lines:
        lines = summary.split('\n')
        ng_words = ['Amazon', 'URL', 'ãƒªãƒ³ã‚¯', 'ä¸ƒç€¬', 'å•†å“ç´¹ä»‹', 'ç‰¹å…¸',
                    'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚«ãƒ¼ãƒ‰', 'Success Book', 'å‹•ç”»', 'æ¦‚è¦æ¬„',
                    'ãŠã™ã™ã‚é †ã§ã¯ãªã„', 'ã‚¢ã‚½ã‚·ã‚¨ã‚¤ãƒˆ', 'è³¼å…¥ãƒšãƒ¼ã‚¸',
                    'æä¾›:', 'Mainichi Eikaiwa', 'è©•åˆ¤', 'ãŠã™ã™ã‚æœ¬', 'å‡ºæ¼”æœ¬',
                    'å‚è€ƒæœ¬', 'ãŠå‹§ã‚æœ¬', 'TOEIC', 'å‹‰å¼·æœ¬', 'ã‚ªãƒ¼ãƒ‡ã‚£ãƒ–ãƒ«',
                    'Audible', 'Kindle', 'Udemy', 'æ‰‹å¸³', 'ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼',
                    'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è‹±ä¼šè©±', 'AQUES', 'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²', 'LOWYAã®',
                    'Meta Quest', 'Kindleç«¯æœ«', 'æœ¬æ£šãƒ‡ã‚¹ã‚¯', 'ã¯ã“ã¡ã‚‰',
                    'ã‚¿ã‚¤ãƒãƒ¼', 'ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼', 'ãƒœãƒ¼ãƒ‰ã‚²ãƒ¼ãƒ ', 'ã‹ã£ã•',
                    'ãƒ†ãƒ©ãƒ˜ãƒ«ãƒ„', 'ã‚¤ãƒ¤ãƒ›ãƒ³', 'ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰', 'ãƒã‚¦ã‚¹',
                    'ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤', 'ãƒ¢ãƒ‹ã‚¿ãƒ¼', 'ãƒã‚§ã‚¢', 'ãƒ©ã‚¤ãƒˆä»˜ã',
                    'é‡‘ãƒ•ãƒ¬', 'ã‚­ã‚¯ã‚¿ãƒ³', 'ã§ã‚‹1000å•', 'å…¬å¼å•é¡Œé›†',
                    'ç²¾é¸å•é¡Œé›†', 'ç²¾é¸æ¨¡è©¦']

        for i, line in enumerate(lines):
            line_stripped = line.strip()
            amazon_match = re.search(r'https?://amzn\.to/[A-Za-z0-9]+', line_stripped)
            if not amazon_match:
                continue

            title_candidate = None
            amazon_url = amazon_match.group(0)

            # å½¢å¼A: amzn.toã®å‰ã«ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹ï¼ˆåŒä¸€è¡Œï¼‰
            before_url = line_stripped[:amazon_match.start()].strip()
            if before_url and not before_url.startswith('http'):
                title_candidate = before_url
            # å½¢å¼B: amzn.toã ã‘ã®è¡Œ â†’ å‰ã®è¡ŒãŒã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè‘—è€…ãƒ»å‡ºç‰ˆç¤¾è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            elif line_stripped == amazon_url and i > 0:
                for j in range(i-1, max(i-5, -1), -1):
                    prev_line = lines[j].strip()
                    if not prev_line or prev_line.startswith('http'):
                        break
                    # è‘—è€…ãƒ»å‡ºç‰ˆç¤¾ãªã©ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç©ºç™½å…¥ã‚Šã‚‚å¯¾å¿œ: ã€Œè‘—ã€€è€…ã€ã€Œç›£ã€€è¨³ã€ï¼‰
                    if re.match(r'^(è‘—[\sã€€]*è€…|ç›£[\sã€€]*è¨³|å‡ºç‰ˆç¤¾|å‡ºç‰ˆ|ç™ºè¡Œ|ç™ºå£²æ—¥|ä¾¡æ ¼|å®šä¾¡)[\s\u200f\u200e]*[ï¼š:.\sã€€]', prev_line):
                        continue
                    # æ‹¬å¼§ã ã‘ã®è£œè¶³è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä¾‹: ã€Œ(æ—¥æœ¬èªç‰ˆ)ã€ã€Œï¼ˆå®Œå…¨ç‰ˆï¼‰ã€ï¼‰
                    if re.match(r'^[ï¼ˆ(].+[ï¼‰)]$', prev_line):
                        continue
                    # è‘—è€…è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä¾‹: ã€Œã‚¨ãƒŸãƒ³ãƒ»ãƒ¦ãƒ«ãƒã‚º (è‘—)ã€ï¼‰
                    if re.search(r'[ï¼ˆ(]è‘—[ï¼‰)]', prev_line) and 'ã€' not in prev_line and 'ã€Œ' not in prev_line:
                        continue
                    title_candidate = prev_line
                    break

            if not title_candidate:
                continue

            # NGãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            if any(ng in title_candidate for ng in ng_words):
                continue

            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            cleaned = re.sub(r'^[*\sãƒ»â€»â¤ï¸ğŸ“•ğŸ“—ğŸ“˜ğŸ“™ğŸ”½â–½â†“]+', '', title_candidate).strip()
            # æ‹¬å¼§ä»˜ãã®è£œè¶³ã‚’é™¤å»: ã€Œã‚¿ã‚¤ãƒˆãƒ«(Amazon)ã€â†’ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€
            cleaned = re.sub(r'[ï¼ˆ(](?:Amazon|Amazonãƒªãƒ³ã‚¯|ã‚¢ãƒã‚¾ãƒ³)[ï¼‰)]$', '', cleaned).strip()
            # ã€ã€ã€Œã€ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯å¤–ã™
            if cleaned.startswith('ã€') and cleaned.endswith('ã€'):
                cleaned = cleaned[1:-1]
            if cleaned.startswith('ã€Œ') and cleaned.endswith('ã€'):
                cleaned = cleaned[1:-1]

            if cleaned and len(cleaned) > 2:
                results.append({
                    "title": cleaned,
                    "author": None,
                    "publisher": None,
                })

        if results:
            return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³5: ã‚¢ãƒã‚¿ãƒ­ãƒ¼ã€Œæ›¸ç±ã®è³¼å…¥ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    abataro_section = re.search(
        r'(?:ã€æ›¸ç±ã®è³¼å…¥ã€‘|â–¼æ›¸ç±ã®è³¼å…¥)\s*\n?(.*?)(?=\nâ–¼|\n\n\n|\Z)', summary, re.DOTALL
    )
    if abataro_section:
        section_text = abataro_section.group(1)
        lines = section_text.strip().split('\n')
        seen_titles = set()
        is_first = True
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            # éæ›¸ç±è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if not line or line.startswith('http') or 'ã‚¨ãƒƒã‚»ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ' in line or 'ç°¡æ˜“ç‰ˆ' in line:
                i += 1
                continue
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ã‚µãƒ¼ãƒ“ã‚¹å®£ä¼ãƒ»ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãƒ»çµµæ–‡å­—ä»˜ãå‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if (line.startswith('ã€') or line.startswith('#') or
                'Audible' in line or 'Kindle' in line or 'amzn.to' in line or
                line.startswith('ğŸ“—') or line.startswith('ğŸ“•') or
                'æœ¬ã‚’è´ã' in line or 'é–¢é€£å‹•ç”»' in line or
                'åˆ†è§£èª¬' in line or 'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²' in line or
                'SNS' in line or 'Twitter' in line or 'Instagram' in line or
                'OUTPUTèª­æ›¸è¡“' in line):
                i += 1
                continue
            line = re.sub(r'^ãƒ»\s*', '', line)
            book_match = re.match(r'(.+?)(?:[ï½œ|](.+?))?(?:[ï¼ˆ(](.+?)[ï¼‰)])?$', line)
            if book_match:
                title = book_match.group(1).strip()
                author = book_match.group(2).strip() if book_match.group(2) else None
                publisher = book_match.group(3).strip() if book_match.group(3) else None
                if title not in seen_titles:
                    seen_titles.add(title)
                    results.append({
                        "title": title,
                        "author": author,
                        "publisher": publisher,
                        "_is_first": is_first,
                    })
                is_first = False
            i += 1
        if results:
            return results

    return results


def clean_book_title(title):
    """ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰è‘—è€…åãƒ»å‡ºç‰ˆç¤¾ãªã©ã®ä»˜åŠ æƒ…å ±ã‚’é™¤å»"""
    if not title:
        return title
    title = title.strip()

    # å…ˆé ­ã®çµµæ–‡å­—ãƒ»è¨˜å·ãƒ»ä¸¸æ•°å­—ã‚’é™¤å»ï¼ˆğŸ“šğŸ“—â–¶ï¸â—‰â‘ â‘¡ç­‰ï¼‰
    # U+FE0E/U+FE0F (variation selector) ã‚‚å«ã‚ã¦é™¤å»
    title = re.sub(r'^[ğŸ“šğŸ“—ğŸ“•ğŸ“˜ğŸ“™ğŸ“–ğŸ”½â–¶â–·â—‰â—â—‹â—â– â–¡â–ªâ–«â˜…â˜†âœ…âœ“â†’â–ºâ¤ğŸ”¶ğŸ”·ğŸ’¡ğŸ¯ğŸ“Œâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³\ufe0e\ufe0f]+[\sã€€.ï¼‰)ã€]*', '', title)

    # æœ«å°¾ã®çµµæ–‡å­—ãƒ»è¨˜å·ãƒ»ä¸¸æ•°å­—ã‚’é™¤å»
    title = re.sub(r'[\sã€€.ã€,ï¼Œ]*[ğŸ“šğŸ“—ğŸ“•ğŸ“˜ğŸ“™ğŸ“–ğŸ”½â–¶â–·â—‰â—â—‹â—â– â–¡â–ªâ–«â˜…â˜†âœ…âœ“â†’â–ºâ¤ğŸ”¶ğŸ”·ğŸ’¡ğŸ¯ğŸ“Œâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³\ufe0e\ufe0f]+$', '', title)

    # æœ«å°¾ã®æ‹¬å¼§ï¼ˆ...ï¼‰ã‚„ï¼ˆ...ï¼‰ã‚’é™¤å»
    title = re.sub(r'[\sã€€]*[ï¼ˆ(][^ï¼‰)]*[ï¼‰)]$', '', title)

    # ã€Œæ›¸ç±ï¼šã€ã€Œè‘—æ›¸ï¼šã€ç­‰ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
    title = re.sub(r'^(æ›¸ç±|è‘—æ›¸)[ï¼š:]\s*', '', title)

    # ã€Œãƒ›ãƒƒãƒˆâ™¨ã€ã€Œã‚¢ã‚¤ã‚¹ğŸ§Šã€ç­‰ã‚’å‰Šé™¤
    title = re.sub(r'ãƒ›ãƒƒãƒˆâ™¨ï¸?', '', title)
    title = re.sub(r'ã‚¢ã‚¤ã‚¹ğŸ§Š?', '', title)

    # ã€ŒKindleç‰ˆã€ã€Œå˜è¡Œæœ¬ã€ç­‰ã®å½¢æ…‹è¡¨è¨˜ã‚’å‰Šé™¤
    title = re.sub(r'\s*(Kindleç‰ˆ|å˜è¡Œæœ¬|æ–‡åº«|æ–°æ›¸|ãƒãƒ¼ãƒ‰ã‚«ãƒãƒ¼)\s*$', '', title)
    title = re.sub(r'\s*(Kindleç‰ˆ|å˜è¡Œæœ¬|æ–‡åº«|æ–°æ›¸|ãƒãƒ¼ãƒ‰ã‚«ãƒãƒ¼)\s*', ' ', title).strip()

    # ã€ã‚¿ã‚¤ãƒˆãƒ«ã€â†’ ã€ã€å†…ã ã‘æŠ½å‡º
    m = re.search(r'ã€(.+?)ã€', title)
    if m:
        return m.group(1).strip()

    # ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ï¼‹å¾Œç¶šãƒ†ã‚­ã‚¹ãƒˆ â†’ ã€Œã€å†…ã ã‘æŠ½å‡º
    # ãƒã‚¹ãƒˆã—ãŸã€Œã€ã«ã‚‚å¯¾å¿œ: ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€Œã‚µãƒ–ã€ç¶šãã€è‘—è€…å
    bracket_match = re.match(r'^ã€Œ(.+)ã€(.*)$', title)
    if bracket_match:
        inner = bracket_match.group(1).strip()
        after = bracket_match.group(2).strip()
        # å¾Œã‚ãŒç©º or è‘—è€…åã‚‰ã—ã„ãƒ†ã‚­ã‚¹ãƒˆ â†’ ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
        if not after or not after.startswith('ã€Œ'):
            return inner

    # é€”ä¸­ã«ã€Œã€ãŒã‚ã‚‹å ´åˆ: è‘—è€…ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€
    m = re.search(r'ã€Œ(.+?)ã€', title)
    if m:
        inner = m.group(1).strip()
        before = title[:m.start()].strip()
        after = title[m.end():].strip()
        if re.match(r'^(è‘—æ›¸|è‘—è€…)', before) or (after and re.match(r'[â–·â–¶â†’(ï¼ˆ]', after)):
            return inner

    # æœ«å°¾ã®ã€Œï¼ˆè‘—è€…åè‘—ï¼‰ã€ã€Œ(è‘—è€…åè‘—)ã€ã‚’é™¤å»
    title = re.sub(r'[ï¼ˆ(].+?è‘—[ï¼‰)]\s*$', '', title).strip()

    # æœ«å°¾ã®ã€Œï¼ˆå‡ºç‰ˆç¤¾åï¼‰ã€ã¨å¾Œç¶šã®è‘—è€…åç­‰ã‚’é™¤å»ï¼ˆæ–‡åº«ãƒ»æ–°æ›¸ãƒ»é¸æ›¸ãªã©ï¼‰
    title = re.sub(r'[ï¼ˆ(](å¹»å†¬èˆæ–‡åº«|æ–°æ½®æ–°æ›¸|è¬›è«‡ç¤¾æ–‡åº«|è§’å·æ–‡åº«|æ–‡æ˜¥æ–‡åº«|é›†è‹±ç¤¾æ–‡åº«|PHPæ–°æ›¸|ä¸­å…¬æ–°æ›¸|å²©æ³¢æ–°æ›¸|ã¡ãã¾æ–°æ›¸|å…‰æ–‡ç¤¾æ–°æ›¸|æœæ—¥æ–°æ›¸|SBæ–°æ›¸|ç¥¥ä¼ç¤¾æ–°æ›¸|è¬›è«‡ç¤¾ç¾ä»£æ–°æ›¸|è¬›è«‡ç¤¾\+Î±æ–°æ›¸|ãƒãƒ¤ã‚«ãƒ¯æ–‡åº«|å‰µå…ƒæ¨ç†æ–‡åº«|PHPæ–‡åº«|ã ã„ã‚æ–‡åº«|çŸ¥çš„ç”Ÿãã‹ãŸæ–‡åº«|ä¸‰ç¬ æ›¸æˆ¿)[ï¼‰)].*$', '', title).strip()

    # ã€Œæ¸¡é‚‰æ­£è£• è‘—ã€ã‚¿ã‚¤ãƒˆãƒ«ã€ã€ãƒ‘ã‚¿ãƒ¼ãƒ³
    m = re.match(r'.+?\s+è‘—\s*ã€(.+?)ã€', title)
    if m:
        return m.group(1).strip()

    return title


def is_valid_book_title(title):
    """æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦æœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    if not title or not isinstance(title, str):
        return False

    title = title.strip()

    # çŸ­ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’é™¤å¤–ï¼ˆ3æ–‡å­—ä»¥ä¸‹ï¼‰
    if len(title) <= 3:
        return False

    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§å§‹ã¾ã‚‹ã‚‚ã®ï¼ˆç›®æ¬¡ï¼‰ã‚’é™¤å¤–ï¼ˆ00:00 å½¢å¼ï¼‰
    if re.match(r'^\d{1,2}:\d{2}', title):
        return False

    # çµµæ–‡å­—ã§å§‹ã¾ã‚‹ã‚‚ã®ã‚’é™¤å¤–
    emoji_starts = ['ğŸ“š', 'ğŸ“—', 'ğŸ“•', 'ğŸ“˜', 'ğŸ“™', 'â–¼', 'ã€', 'â– ', 'â—', 'â—‰', 'â—', 'â—‹', 'ãƒ»', 'â€»']
    if any(title.startswith(emoji) for emoji in emoji_starts):
        return False

    # NGãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã‚„å®£ä¼ï¼‰ã‚’é™¤å¤–
    ng_words = [
        # 'ãã®ä»–',
        # 'ãŠã™ã™ã‚å‹•ç”»',
        # 'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²',
        # 'é–¢é€£å‹•ç”»',
        # 'å‹•ç”»ä¸€è¦§',
        # 'SNS',
        # 'Twitter',
        # 'Instagram',
        # 'LINE',
        # 'ã‚¨ãƒƒã‚»ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ',
        # 'ç°¡æ˜“ç‰ˆ',
        'Audibleç‰ˆ',
        'Kindleç«¯æœ«',
        # 'æœ¬ã‚’è´ã',
        # 'åˆ†è§£èª¬',
        # 'è¦ç´„',
        # 'è§£èª¬',
        # 'ã¾ã¨ã‚',
        # 'ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ',
        # 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³',
        # 'ç„¡æ–™',
        # 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«',
        # 'ãŠå•ã„åˆã‚ã›',
        # 'ãƒ¡ãƒ³ãƒãƒ¼ã‚·ãƒƒãƒ—',
        # 'ã‚µãƒ–ãƒãƒ£ãƒ³ãƒãƒ«',
        # ä¸ƒç€¬ã‚¢ãƒªãƒ¼ã‚µé–¢é€£ã®å®£ä¼ã‚’é™¤å¤–
        'ä¸ƒç€¬åˆ¶ä½œ',
        'å•†å“ç´¹ä»‹',
        'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚«ãƒ¼ãƒ‰',
        'Success Book',
        'Your Success',
        'è³¼å…¥ãƒšãƒ¼ã‚¸',
        'ç‰¹å…¸',
        # 'ãŠã™ã™ã‚é †ã§ã¯ãªã„',
        'æ¦‚è¦æ¬„',
        # 'ãƒ‡ã‚¸ã‚¿ãƒ«ç‰ˆ',
        'å†Šå­ç‰ˆ',
        # YouTuberè‡ªè‘—ã®å®£ä¼ã‚’é™¤å¤–
        'OUTPUTèª­æ›¸è¡“',
        'é€±åˆŠSPA',
        'äººç”Ÿã‚’å¤‰ãˆã‚‹ å“²å­¦è€…ã®è¨€è‘‰366',
        'ç¬é–“è‹±ä½œæ–‡',
        "å‘ªè¡“å»»æˆ¦",
        # åŒ–ç²§å“ãƒ»ç¾å®¹ç”¨å“ã‚’é™¤å¤–
        'Etude House BB cream',
        'Biooil',
        'Biore Sunscreen',
        "Visse's stick concealer",
        'Blush, eyeshadow pallet',
        "Visse's powder foundation",
        'Eyeblow powder',
        "Eyebrow's mascara",
        'lip balm',
        "Visse's powder blush",
        'IVY lip stick PK-300',
        'Hair Spray',
        'Panasonic 32mm hair iron ionity',
        'Find out more about Star Wars',
        'Alba',
        'BOH',
        'cosnori',
        "KINUAMI",
        'STRONG',
        'LUSH',
        "ï¼†WELL",
        "Kiva",
        "Haddrell",
        'Mainichi Eikaiwa',
        # ãã®ä»–å•†å“ã‚’é™¤å¤–
        'ãƒ•ã‚£ãƒ¼ãƒãƒ¼ãƒ’ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼',
        'ã‚³ãƒ¼ãƒ’ãƒ¼è±†ï¼ˆæˆåŸçŸ³äº•ã®ï¼‰',
        'ãƒã‚­ã‚·ãƒ ã‚³ãƒ¼ãƒ’ãƒ¼ã€€ãƒ‡ã‚«ãƒ•ã‚§',
        'ã‚·ãƒªã‚«æ°´ãƒ¬ã‚¸ãƒ¼ãƒŠ',
        'ãºã‚“ã¦ã‚‹',
        'ãƒ¨ã‚¬ãƒãƒƒãƒˆ',
        'ã‚·ãƒªã‚«',
        'VOX',
        'ã‚³ãƒ¼ãƒ’ãƒ¼ãƒ¡ãƒ¼ã‚«ãƒ¼',
        'iPad ã€€Pro.',
        'è›å…‰ãƒšãƒ³',
        'Mark +è›å…‰ãƒšãƒ³',
        'å¤šæ©Ÿèƒ½ãƒœãƒ¼ãƒ«ãƒšãƒ³',
        'iPadã‚«ãƒãƒ¼',
        'è“‹ãŒè¦‹ãˆã‚‹ã”é£¯é‡œ',
        'ãƒšãƒ¼ãƒ‘ãƒ¼ãƒ©ã‚¤ã‚¯ãƒ•ã‚£ãƒ«ãƒ ',
        'å­£ç¯€ã®çˆç²',
        'ãƒ¦ãƒ«ãƒ èŒ¶',
        'ãƒ˜ã‚¢ã‚¹ãƒ—ãƒ¬ãƒ¼',
        'ã®ã©ã¬ãƒ¼ã‚‹',
        'ãƒ‡ãƒ‹ãƒ ã®ã‚„ã¤',
        'è¶³ãƒãƒƒã‚µãƒ¼ã‚¸',
        'ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ã‚·ãƒ¼ãƒˆ',
        'ã‚¤ãƒ³ãƒ‰æ˜ ç”»RR',
        'ãƒãƒ¬ãƒƒãƒˆã‚¸ãƒ£ãƒ¼ãƒŠãƒ«',
        # é£Ÿå“ãƒ»æ—¥ç”¨å“ã‚’é™¤å¤–
        'ã¨ã‚“ã“ã¤',
        'ç„ç±³ãƒ©ãƒ¼ãƒ¡ãƒ³',
        'ã“ã‚“ã«ã‚ƒããƒ©ãƒ¼ãƒ¡ãƒ³',
        'å¤§è±†éºº',
        'å¤§è‡ªç„¶ãƒ©ãƒ¼ãƒ¡ãƒ³',
        'ç„¡é¦™æ–™',
        'ã‚¤ã‚ªãƒ³æ¶ˆè‡­ãƒ—ãƒ©ã‚¹',
        'ã‚†ãšæ²¹',
        'UVã‚¤ãƒ‡ã‚¢ãƒ—ãƒ­ãƒ†ã‚¯ã‚·ãƒ§ãƒ³ãƒˆãƒ¼ãƒ³ã‚¢ãƒƒãƒ—',
        'ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒ»ãƒ•ã‚§ã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ»ã‚«ãƒ•ã‚§ã‚¤ãƒ³ãƒ¬ã‚¹ãƒ»ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ãƒˆã‚³ãƒ¼ãƒ’ãƒ¼',
        'ã‚¢ã‚¤ãƒã‚¹ã‚¯',
        'ã‚¹ãƒã‚¤ãƒ«ã‚¶ãƒ¡ãƒ‡ã‚£ã‚«ãƒ«Aãƒ»DX',
        'ã¶ã©ã†å±±æ¤’',
        'ãƒ—ãƒ¼ã‚¢ãƒ«èŒ¶',
        'ã‚¢ãƒ³ãƒ‰ã‚°ãƒƒãƒ‰ãƒŠã‚¤ãƒˆè–¬ç”¨å…¥æµ´å‰¤',
        'ãƒ‡ã‚ªãƒ‰ãƒ©ãƒ³ãƒˆã‚½ãƒ¼ãƒ—',
        'UVãƒ—ãƒ­ãƒ†ã‚¯ãƒˆ',
        'ç„¼è‚‰ã®ãŸã‚Œ',
        'ã‚¦ã‚£ãƒ«ã‚­ãƒ³ã‚½ãƒ³',
        'ã»ã†ã˜èŒ¶',
        'ã‚ªãƒ«ãƒŠ ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ ã‚·ãƒ£ãƒ³ãƒ—ãƒ¼',
        'ãƒ‘ã‚­ã‚¹ã‚¿ãƒ³ç”£',
        'ç´”ã‚Šã‚“ã”é…¢',
        'ç´”ãƒªãƒ³ã‚´é…¢',
        'ã‚¯ã‚¤ãƒƒã‚¯ãƒ«ãƒ¯ã‚¤ãƒ‘ãƒ¼',
        'ãƒ“ã‚ªã‚¹ãƒªãƒ¼',
        'ãƒŸãƒ¤ãƒªã‚µãƒ³',
        'ã¯ã¨ã‚€ã',
        'ã‚ˆã‚‚ã',
        'ã‚¨ã‚­ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚¸ãƒ³ãƒ»ã‚ªãƒªãƒ¼ãƒ–ã‚ªã‚¤ãƒ«',
        'ã‚¶ãƒ—ãƒ­ã‚°ãƒ©ã‚¹ãƒ•ã‚§ãƒƒãƒ‰ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³',
        'ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒ•ã‚§ã‚¢ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ãƒˆã‚³ãƒ¼ãƒ’ãƒ¼',
        'ã²ãã‚ã‚Šç´è±†',
        'ã‚°ã‚¡ãƒèŒ¶',
        'ä½åˆ†å­ã‚³ãƒ©ãƒ¼ã‚²ãƒ³',
        'ã‚¼ãƒ©ãƒãƒ³',
        'Lamicall',
        'Tapo',
        'è±¡å°ã®ç‚èˆç‚Šã',
        'ã‚°ãƒ¬ã‚´ãƒªãƒ¼',
        'AirPods',
        'ã‚²ãƒ¼ãƒŸãƒ³ã‚°',
        'pcãƒ¡ã‚¬ãƒ',
        'ã‚¨ãƒ«ã‚´ãƒˆãƒ­ãƒ³',
        'ãƒ•ã‚§ã‚¤ã‚¯è¦³è‘‰æ¤ç‰©',
        'ã¤ã°ã‚ã®ãƒãƒ¼ãƒˆ',
        'ãƒ„ãƒãƒ¡ãƒãƒ¼ãƒˆ',
        'é€±åˆŠ',
        'æ˜¥ãŒè¦‹ã¤ã‹ã‚‰ãªã„',
        'é‹(ãƒ†ã‚£â—ãƒ¼ãƒ«ã‚ˆã‚Šå®‰ã„ã—å¯æ„›ã„ï¼‰',
        'ãƒãƒ§ãƒ¼ãƒ¤ã®æ¢…é…’',
        'ãƒ‰ãƒ©ã‚¤ãƒ¤ãƒ¼ã‚¹ã‚¿ãƒ³ãƒ‰',
        'ãŠã™ã™ã‚ã®ã€Œã‚·ãƒ¥ãƒ¼ã‚ºãƒ©ãƒƒã‚¯ã€',
        'ãŠã™ã™ã‚ã®ã€Œæ°´åˆ‡ã‚Šè¢‹ã€',
        'æ­¯ãƒ–ãƒ©ã‚·ãƒ›ãƒ«ãƒ€ãƒ¼',
        'ãƒ†ã‚£ãƒ¼ãƒãƒƒã‚¯',
        'ã‚«ãƒ•ã‚§ã‚¤ãƒ³ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ’ãƒ¼',
        'ã‚«ãƒ•ã‚§ã‚¤ãƒ³ãƒ¬ã‚¹ç´…èŒ¶',
        'ãƒ¬ãƒ³ã‚¸ã§å‡ºæ¥ã¡ã‚ƒã†',
        'ã‚¢ã‚¤ãƒªã‚¹ã‚ªãƒ¼ãƒ¤ãƒ',
        'ã‚ã—ã‚†ã³é–‹ã',
        'å›½ç”£æœ‰æ©Ÿæ ½åŸ¹ãƒŸãƒ‹ãƒ’ã‚«ãƒª',
        'ç„¡è¾²è–¬ãƒ’ãƒãƒ’ã‚«ãƒª',
        'ç‰¹åˆ¥æ ½åŸ¹ç±³',
        'ãƒ‡ã‚¶ã‚¤ãƒ‹ãƒ³ã‚°ã‚¢ã‚¤ãƒ–ãƒ­ã‚¦',
        'æ›æ°—æ‰‡ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼',
        'æ°¸å²¡é£Ÿå“',
        'ç©ºæ°—æ¸…æµ„æ©Ÿ',
        'ãƒãƒ¬ãƒƒãƒˆã‚¸ãƒ£ãƒ¼ãƒŠãƒ«',
        'ã‚¹ãƒãƒ›ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ™ãƒ«ãƒˆ',
        'ãƒŸãƒ³ãƒˆè‰²ã®æ–¹',
        'é›ªå¡©',
        'æµ·äººã®è—»å¡©',
        'ç‰¹åˆ¥æ ½åŸ¹ç±³',
        'ä¸‰é‡çœŒç”£',
        'é’æ£®è¾²ç”£',
        'ãƒªãƒ³ã‚¹',
        'GABAN',
        'é¯–ç¼¶',
        'ç¼¶è©°',
        'ã«ã‚“ã˜ã‚“ã—ã‚Šã—ã‚Š',
        "ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ–ãƒƒã‚¯ãŒç„¡æ–™ã§èã‘ã¾ã™",
        "ãƒ˜ãƒƒãƒ‰ã‚»ãƒƒãƒˆ",
        "ãƒãƒŒã‚«ãƒãƒ‹ãƒ¼",
        "ã”ã¼ã†èŒ¶",
        "ãƒ’ãƒãƒ©ãƒ¤ãƒ”ãƒ³ã‚¯ã‚½ãƒ«ãƒˆ",
        "ã‚°ãƒ©ã‚¹ãƒ•ã‚§ãƒƒãƒ‰ã‚®ãƒ¼",
        "ãƒãƒ¼ã‚¸ãƒ³ã‚³ã‚³ãƒŠãƒƒãƒ„ã‚ªã‚¤ãƒ«",
        "â™¨",
        "ãƒã‚¤ã‚»ãƒªã‚¢",
        "ãƒ‡ãƒƒãƒ‰ã‚ªãƒ–ã‚¦ã‚£ãƒ³ã‚¿ãƒ¼",
        "ã‚·ãƒ£ãƒ³ãƒˆãƒªãƒœãƒ‡ã‚£",
        "ãƒ•ãƒƒãƒˆãƒãƒƒã‚µãƒ¼ã‚¸ãƒ£ãƒ¼",
        "1æ—¥ã§ãœã‚“ã¶å­¦ã¹ã‚‹ æˆåŠŸè€…ã®æ•™ãˆãƒ™ã‚¹ãƒˆã‚»ãƒ©ãƒ¼100å†Š",
        "é­”æ€§ã‚Œã®æ–¹ã‚‚å¥½ã",
        "ã‚³ãƒ¼ãƒ’ãƒ¼è±†",
        "DIME",
        "ãƒ—ãƒ©ã‚ºãƒè§£é›¢æ°´",
        "ã‚°ãƒ¬ãƒ¼ã‚‚ã‚ã‚‹ã¿ãŸã„",
        "ãƒ­ãƒ‡ã‚£ã‚¢ã®æ–¹",
        "å¤šè´å¤šèª­ãƒã‚¬ã‚¸ãƒ³",
        "ã§ã¦ã„ã‚‹ã‚ˆã†ã§ã™ã­",
        "ã‚ã¾ã‚Šå£²ã£ã¦ãªã„",
        "The Rules of Everything Rules",
        "è„³ç§‘å­¦è€…ã€€ä¸­é‡ä¿¡å­ã€€ç·ã¾ã¨ã‚",
        "ç›®æ¨™ã‚’ç«‹ã¦ã¦ã‚‚ã€ãªã‹ãªã‹è¡Œå‹•ã«ç§»ã›ãªã„",
        # ã‚«ãƒ¼ãƒ‰ã‚²ãƒ¼ãƒ ç­‰ã®å•†å“ã‚’é™¤å¤–
        "XENO",
        "é€šå¸¸ç‰ˆï¼š",
        "è±ªè¯ç‰ˆï¼š",
    ]

    for ng in ng_words:
        if ng in title:
            return False

    # ã€Œæœ¬ã€ã ã‘ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’é™¤å¤–
    if title in ['æœ¬', 'æ›¸ç±', 'å›³æ›¸', 'book', 'books']:
        return False

    # ã€Œã€‡æœ¬ã‚»ãƒƒãƒˆã€ã®ã‚ˆã†ãªå•†å“è¡¨è¨˜ã‚’é™¤å¤–
    if re.search(r'\d+(æœ¬|å†Š)ã‚»ãƒƒãƒˆ', title):
        return False

    # å®¹é‡è¡¨è¨˜ã‚’å«ã‚€å•†å“ã‚’é™¤å¤–ï¼ˆä¾‹: 145gã€250mlã€1.5kgï¼‰
    # \bã¯å…¨è§’æ–‡å­—ã®å‰ã§æ©Ÿèƒ½ã—ãªã„ãŸã‚ã€å¦å®šå…ˆèª­ã¿ã§è‹±å­—ä»¥å¤–ã‚’è¨±å®¹
    if re.search(r'\d+(\.\d+)?\s*(g|kg|ml|mL|L)(?![a-zA-Z])', title, re.IGNORECASE):
        return False

    # ä¾¡æ ¼/å®¹é‡è¡¨è¨˜ã‚’é™¤å¤–ï¼ˆä¾‹: å††/gï¼‰
    if re.search(r'å††/(g|kg|ml|mL|L)\b', title, re.IGNORECASE):
        return False

    # è‘—è€…åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–: ã€Œã€‡ã€‡(è‘—)ã€ã€Œã€‡ã€‡ï¼ˆè‘—ï¼‰ã€ã€Œã€‡ã€‡ã•ã¾ã€ã®ã¿ã®è¡Œ
    if re.search(r'[ï¼ˆ(]è‘—[ï¼‰)]', title) and 'ã€' not in title and 'ã€Œ' not in title:
        return False
    if re.match(r'^[\w\sãƒ»ã€€]+ã•ã¾[\sã€€]*$', title):
        return False

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã‚’é™¤å¤–: ã€Œè‘—è€…ï¼šã€‡ã€‡ã€ã€Œå‡ºç‰ˆç¤¾ï¼šã€‡ã€‡ã€ã€Œå‡ºç‰ˆç¤¾ã€€ã€‡ã€‡ã€ã€Œè‘—ã€€è€…ã€ã€Œç·¨é›†ã€€ã€‡ã€‡ã€
    if re.match(r'^(è‘—[\sã€€]*è€…|å‡ºç‰ˆç¤¾|å‡ºç‰ˆ|ç™ºè¡Œ|æ›¸ç±|ç·¨é›†|ç¿»è¨³|ç›£ä¿®)[\s\u200f\u200e]*[ï¼š:.ã€€\s]', title):
        return False
    # ã€Œã€‡ã€‡ (ç·¨é›†)ã€ã€Œã€‡ã€‡ (ç·¨è‘—)ã€ã€Œã€‡ã€‡ (ç¿»è¨³)ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–
    if re.search(r'[ï¼ˆ(](ç·¨é›†|ç·¨è‘—|ç›£ä¿®|ç¿»è¨³)[ï¼‰)][\s]*$', title):
        return False

    # è‘—ä½œæ¨©ãƒ»è¨±è«¾è¡¨è¨˜ã‚’é™¤å¤–
    if re.search(r'(è¨±è«¾ã‚’å¾—ã¦|é…ä¿¡ã—ã¦ãŠã‚Šã¾ã™|æä¾›ã§ãŠé€ã‚Š|ã‚¿ã‚¤ã‚¢ãƒƒãƒ—)', title):
        return False
    # èª¬æ˜æ–‡ãƒ»æ¡ˆå†…æ–‡ã‚’é™¤å¤–
    if title.startswith('æœ¬å‹•ç”»ã¯'):
        return False
    if re.search(r'(ã‚¢ãƒã‚¾ãƒ³ã§è³¼å…¥|Amazonã§è³¼å…¥|è³¼å…¥ã§ãã¾ã™|è³¼å…¥ã¯ã“ã¡ã‚‰)', title):
        return False


    # URLã£ã½ã„ã‚‚ã®ã‚’é™¤å¤–
    if 'http' in title.lower() or '.com' in title.lower():
        return False

    # å…¨ã¦è¨˜å·ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’é™¤å¤–
    if all(not c.isalnum() for c in title):
        return False

    # YouTuberåãŒå…¥ã£ã¦ã„ã‚‹ã‚‚ã®ã‚’é™¤å¤–ï¼ˆè‡ªè‘—å®£ä¼ã®å¯èƒ½æ€§ï¼‰
    youtuber_names = ['ã‚¢ãƒã‚¿ãƒ­ãƒ¼', 'ã‚µãƒ©ã‚¿ãƒ¡', 'æœ¬è¦ç´„ãƒãƒ£ãƒ³ãƒãƒ«', 'å­¦è­˜ã‚µãƒ­ãƒ³', 'ãƒ•ã‚§ãƒ«ãƒŸ', 'ä¸‰å®…', 'ä¸ƒç€¬', 'ã‚¢ãƒªãƒ¼ã‚µ']
    for name in youtuber_names:
        if name in title:
            return False

    return True


def normalize_title_key(title):
    """è¡¨è¨˜æºã‚Œçµ±ä¸€ç”¨ã®æ­£è¦åŒ–ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
    t = title
    t = re.sub(r'[ã€ã€ã€Œã€]', '', t)
    t = re.sub(r'[ï¼ˆ(](å˜è¡Œæœ¬|æ–‡åº«|æ–°æ›¸|ãƒãƒ¼ãƒ‰ã‚«ãƒãƒ¼|Kindleç‰ˆ)[ï¼‰)]', '', t)
    t = re.sub(r'^(æ”¹è¨‚ç‰ˆ|æ–°ç‰ˆ|æ–°è£…ç‰ˆ|å¢—è£œç‰ˆ|æ±ºå®šç‰ˆ|å®Œå…¨ç‰ˆ)\s*', '', t)
    t = re.sub(r'(æ”¹è¨‚ç‰ˆã§ã™|æ”¹è¨‚ç‰ˆ)$', '', t)
    t = re.sub(r'[\sã€€ã€,ï¼š:]+', '', t)
    t = t.lower()
    return t


def merge_similar_books(all_books):
    """çŸ­ã„ã‚­ãƒ¼ãŒé•·ã„ã‚­ãƒ¼ã®å…ˆé ­ã«å«ã¾ã‚Œã‚‹å ´åˆã€åŒä¸€æ›¸ç±ã¨ã—ã¦çµ±åˆ"""
    keys = sorted(all_books.keys(), key=len)
    merge_map = {}  # short_key -> long_key (çµ±åˆå…ˆ)
    for i, short_key in enumerate(keys):
        if short_key in merge_map or len(short_key) < 5:
            continue
        for long_key in keys[i+1:]:
            if long_key in merge_map:
                continue
            if long_key.startswith(short_key):
                merge_map[short_key] = long_key
                break  # æœ€çŸ­ã®çµ±åˆå…ˆã«çµ±åˆ

    for src_key, dst_key in merge_map.items():
        src = all_books.pop(src_key, None)
        if not src or dst_key not in all_books:
            continue
        dst = all_books[dst_key]
        dst["count"] += src["count"]
        dst["total_views"] += src["total_views"]
        dst["total_likes"] += src["total_likes"]
        dst["videos"].extend(src["videos"])
        dst["_title_variants"].extend(src.get("_title_variants", [src["title"]]))
        if not dst.get("author") and src.get("author"):
            dst["author"] = src["author"]
        if not dst.get("publisher") and src.get("publisher"):
            dst["publisher"] = src["publisher"]


def choose_canonical_title(titles):
    """è¤‡æ•°ã®è¡¨è¨˜æºã‚Œã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æœ€ã‚‚æ­£å¼ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’é¸æŠ"""
    cleaned = [re.sub(r'\s*[ï¼ˆ(](å˜è¡Œæœ¬|æ–‡åº«|æ–°æ›¸|ãƒãƒ¼ãƒ‰ã‚«ãƒãƒ¼|Kindleç‰ˆ)[ï¼‰)]', '', t) for t in titles]
    with_subtitle = [t for t in cleaned if 'ï¼š' in t or ':' in t or 'â€•' in t or 'â€”' in t]
    candidates = with_subtitle if with_subtitle else cleaned
    return max(candidates, key=len)


def generate_amazon_search_url(book_title):
    """æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰Amazonæ¤œç´¢URLã‚’ç”Ÿæˆï¼ˆã‚¢ã‚½ã‚·ã‚¨ã‚¤ãƒˆã‚¿ã‚°ä»˜ãï¼‰"""
    query = urllib.parse.quote(book_title)
    return f"https://www.amazon.co.jp/s?k={query}&i=stripbooks&tag={AMAZON_TRACKING_ID}"


def generate_book_id(title):
    """æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ç”Ÿæˆ"""
    return hashlib.md5(title.encode()).hexdigest()[:12]


# =============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =============================================================================

def load_channels():
    with open(CHANNELS_FILE, "r", encoding="utf-8") as f:
        return json.load(f)["channels"]


def main():
    parser = argparse.ArgumentParser(description="YouTubeå‹•ç”»ã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’æŠ½å‡º")
    parser.add_argument("--full", action="store_true", help="å…¨ä»¶å–å¾—ï¼ˆå·®åˆ†æ›´æ–°ã§ã¯ãªãï¼‰")
    args = parser.parse_args()

    if not YOUTUBE_API_KEY:
        print("ERROR: YOUTUBE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    os.makedirs(DATA_DIR, exist_ok=True)
    channels = load_channels()

    # å·®åˆ†æ›´æ–°ã®çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿
    fetch_state = load_fetch_state() if not args.full else {}
    new_fetch_state = {}

    # æ—¢å­˜ã®æ›¸ç±ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆå·®åˆ†æ›´æ–°ç”¨ï¼‰
    books_file = os.path.join(DATA_DIR, "books.json")
    if not args.full and os.path.exists(books_file):
        with open(books_file, "r", encoding="utf-8") as f:
            existing_books = json.load(f)
        # æ­£è¦åŒ–ã‚­ãƒ¼ã§ãƒãƒƒãƒ—åŒ–
        all_books = {}
        for b in existing_books:
            norm_key = normalize_title_key(b["title"])
            b["_title_variants"] = [b["title"]]
            all_books[norm_key] = b
        print(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(all_books)}ä»¶")
    else:
        all_books = {}

    if args.full:
        print("=== å…¨ä»¶å–å¾—ãƒ¢ãƒ¼ãƒ‰ ===")
    else:
        print("=== å·®åˆ†æ›´æ–°ãƒ¢ãƒ¼ãƒ‰ ===")

    for ch in channels:
        channel_name = ch["name"]
        channel_id = ch["channel_id"]
        print(f"\n=== {channel_name} (ID: {channel_id}) ===")

        # å·®åˆ†æ›´æ–°: å‰å›ã®æœ€æ–°å‹•ç”»æ—¥æ™‚ä»¥é™ã®ã¿å–å¾—
        since = fetch_state.get(channel_id) if not args.full else None
        videos = fetch_all_channel_videos(channel_id, since=since)

        # ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®æœ€æ–°å‹•ç”»æ—¥æ™‚ã‚’è¨˜éŒ²
        if videos:
            latest = max(v["published"] for v in videos)
            new_fetch_state[channel_id] = latest
        elif channel_id in fetch_state:
            new_fetch_state[channel_id] = fetch_state[channel_id]

        for video in videos:
            summary = video.get("summary", "")
            video_title = video.get("title", "")
            book_info_list = extract_book_info_list(summary, video_title)

            if not book_info_list:
                continue

            for book_info in book_info_list:
                book_title = book_info.get("title")
                if not book_title:
                    continue

                # ã‚¿ã‚¤ãƒˆãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆè‘—è€…åãƒ»å‡ºç‰ˆç¤¾ã‚’åˆ†é›¢ï¼‰
                book_title = clean_book_title(book_title)
                book_info["title"] = book_title

                # ã‚¿ã‚¤ãƒˆãƒ«ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                if not is_valid_book_title(book_title):
                    continue

                # è‡ªè‘—å®£ä¼ã‚¹ã‚­ãƒƒãƒ—
                if book_info.get("_is_first") and len(book_info_list) > 1:
                    continue

                # Amazonãƒªãƒ³ã‚¯ã‹ã‚‰å–å¾—ã—ãŸå ´åˆã¯æ—¢ã«amazon_urlãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹
                amazon_url = book_info.get("amazon_url") or generate_amazon_search_url(book_title)

                # è¡¨è¨˜æºã‚Œçµ±ä¸€: æ­£è¦åŒ–ã‚­ãƒ¼ã§åŒä¸€æ›¸ç±ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                norm_key = normalize_title_key(book_title)

                if norm_key not in all_books:
                    all_books[norm_key] = {
                        "id": generate_book_id(norm_key),
                        "title": book_title,
                        "_title_variants": [book_title],
                        "author": book_info.get("author"),
                        "publisher": book_info.get("publisher"),
                        "amazon_url": amazon_url,
                        "count": 0,
                        "total_views": 0,
                        "total_likes": 0,
                        "videos": [],
                    }
                else:
                    # æ–°ã—ã„ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¨˜éŒ²
                    if book_title not in all_books[norm_key]["_title_variants"]:
                        all_books[norm_key]["_title_variants"].append(book_title)
                    # è‘—è€…ãƒ»å‡ºç‰ˆç¤¾ãŒæœªè¨­å®šãªã‚‰è£œå®Œ
                    if not all_books[norm_key]["author"] and book_info.get("author"):
                        all_books[norm_key]["author"] = book_info["author"]
                    if not all_books[norm_key]["publisher"] and book_info.get("publisher"):
                        all_books[norm_key]["publisher"] = book_info["publisher"]

                all_books[norm_key]["count"] += 1
                all_books[norm_key]["total_views"] += video.get("view_count", 0)
                all_books[norm_key]["total_likes"] += video.get("like_count", 0)
                all_books[norm_key]["videos"].append({
                    "video_id": video["video_id"],
                    "video_title": video["title"],
                    "channel": channel_name,
                    "link": video["link"],
                    "published": video["published"],
                    "view_count": video.get("view_count", 0),
                    "like_count": video.get("like_count", 0),
                })

    # --- è¡¨è¨˜æºã‚Œçµ±ä¸€ ---
    # 1. çŸ­ã„ã‚­ãƒ¼ãŒé•·ã„ã‚­ãƒ¼ã«å«ã¾ã‚Œã‚‹å ´åˆã‚’çµ±åˆ
    merge_similar_books(all_books)
    # 2. å„ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æ­£è¦ã‚¿ã‚¤ãƒˆãƒ«ã‚’é¸æŠ
    for book in all_books.values():
        variants = book.pop("_title_variants", [book["title"]])
        if len(variants) > 1:
            canonical = choose_canonical_title(variants)
            book["title"] = canonical
            book["amazon_url"] = generate_amazon_search_url(canonical)

    # --- çµæœè¡¨ç¤º ---
    books_list = list(all_books.values())
    print(f"\n=== æŠ½å‡ºçµæœ ===")
    print(f"æ›¸ç±æ•°: {len(books_list)}")

    # --- æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ã®ãƒãƒ¼ã‚¸ï¼ˆISBNç­‰ã‚’ä¿æŒï¼‰ ---
    books_file = os.path.join(DATA_DIR, "books.json")
    if os.path.exists(books_file):
        with open(books_file, "r", encoding="utf-8") as f:
            existing_books = json.load(f)
        # idã§ãƒãƒƒãƒ—åŒ–
        existing_map = {b["id"]: b for b in existing_books}
        # ã‚¿ã‚¤ãƒˆãƒ«æ­£è¦åŒ–ã‚­ãƒ¼ã§ã‚‚ãƒãƒƒãƒ—åŒ–ï¼ˆIDãŒå¤‰ã‚ã£ãŸå ´åˆã«å¯¾å¿œï¼‰
        existing_by_title = {normalize_title_key(b["title"]): b for b in existing_books}
        # æ–°ãƒ‡ãƒ¼ã‚¿ã«æ—¢å­˜ã®ISBN/ASIN/image_urlç­‰ã‚’ãƒãƒ¼ã‚¸
        for book in books_list:
            # IDã§ãƒãƒƒãƒã€ã¾ãŸã¯ã‚¿ã‚¤ãƒˆãƒ«æ­£è¦åŒ–ã‚­ãƒ¼ã§ãƒãƒƒãƒ
            existing = existing_map.get(book["id"])
            if not existing:
                norm_key = normalize_title_key(book["title"])
                existing = existing_by_title.get(norm_key)
            if existing:
                for key in ["isbn", "asin", "image_url", "publication_date", "openbd_title"]:
                    if existing.get(key) and not book.get(key):
                        book[key] = existing[key]
                # amazon_urlã¯ASINä»˜ãã®ã‚‚ã®ã‚’å„ªå…ˆ
                if existing.get("asin") and "/dp/" in existing.get("amazon_url", ""):
                    book["amazon_url"] = existing["amazon_url"]
                    book["asin"] = existing["asin"]

    # --- JSONç”Ÿæˆ ---

    # books.jsonï¼ˆç´¹ä»‹å›æ•°é †ï¼‰
    books_by_count = sorted(books_list, key=lambda x: x["count"], reverse=True)
    with open(books_file, "w", encoding="utf-8") as f:
        json.dump(books_by_count, f, ensure_ascii=False, indent=2)

    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”¨ã®è»½é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    def make_ranking_entry(book):
        return {
            "id": book["id"],
            "title": book["title"],
            "author": book.get("author"),
            "count": book["count"],
            "total_views": book["total_views"],
            "total_likes": book["total_likes"],
            "amazon_url": book["amazon_url"],
        }

    # rankings.jsonï¼ˆç´¹ä»‹å›æ•°é †ï¼‰
    rankings_count = [make_ranking_entry(b) for b in books_by_count]
    with open(os.path.join(DATA_DIR, "rankings.json"), "w", encoding="utf-8") as f:
        json.dump(rankings_count, f, ensure_ascii=False, indent=2)

    # rankings_views.jsonï¼ˆå†ç”Ÿå›æ•°åˆè¨ˆé †ï¼‰
    books_by_views = sorted(books_list, key=lambda x: x["total_views"], reverse=True)
    rankings_views = [make_ranking_entry(b) for b in books_by_views]
    with open(os.path.join(DATA_DIR, "rankings_views.json"), "w", encoding="utf-8") as f:
        json.dump(rankings_views, f, ensure_ascii=False, indent=2)

    # rankings_likes.jsonï¼ˆã„ã„ã­åˆè¨ˆé †ï¼‰
    books_by_likes = sorted(books_list, key=lambda x: x["total_likes"], reverse=True)
    rankings_likes = [make_ranking_entry(b) for b in books_by_likes]
    with open(os.path.join(DATA_DIR, "rankings_likes.json"), "w", encoding="utf-8") as f:
        json.dump(rankings_likes, f, ensure_ascii=False, indent=2)

    print(f"\n--- TOP20ï¼ˆç´¹ä»‹å›æ•°é †ï¼‰---")
    for i, book in enumerate(books_by_count[:20], 1):
        print(f"  {i}. ã€{book['title']}ã€ (ç´¹ä»‹{book['count']}å› / å†ç”Ÿ{book['total_views']:,} / ã„ã„ã­{book['total_likes']:,})")

    print(f"\n--- TOP10ï¼ˆå†ç”Ÿå›æ•°é †ï¼‰---")
    for i, book in enumerate(books_by_views[:10], 1):
        print(f"  {i}. ã€{book['title']}ã€ (å†ç”Ÿ{book['total_views']:,} / ç´¹ä»‹{book['count']}å›)")

    print(f"\n--- TOP10ï¼ˆã„ã„ã­é †ï¼‰---")
    for i, book in enumerate(books_by_likes[:10], 1):
        print(f"  {i}. ã€{book['title']}ã€ (ã„ã„ã­{book['total_likes']:,} / ç´¹ä»‹{book['count']}å›)")

    # --- å–å¾—çŠ¶æ…‹ã‚’ä¿å­˜ ---
    save_fetch_state(new_fetch_state)

    print(f"\nãƒ‡ãƒ¼ã‚¿ã‚’ {DATA_DIR} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()
