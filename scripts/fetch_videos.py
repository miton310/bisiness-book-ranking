#!/usr/bin/env python3
"""YouTube Data APIã§å…¨å‹•ç”»ã‚’å–å¾—ã—ã€æ›¸ç±æƒ…å ±ã‚’æŠ½å‡ºã—ã¦JSONã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""

import hashlib
import json
import os
import re
import sys
import time
import urllib.parse
import urllib.request

# Amazonãƒªãƒ³ã‚¯ã‹ã‚‰æ›¸ç±æƒ…å ±å–å¾—
from fetch_amazon_info import extract_books_from_amazon_links

DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
CHANNELS_FILE = os.path.join(DATA_DIR, "channels.json")

YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY", "")
if not YOUTUBE_API_KEY:
    # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
    env_path = os.path.join(os.path.dirname(__file__), "..", ".env")
    if os.path.exists(env_path):
        with open(env_path) as f:
            for line in f:
                if line.startswith("YOUTUBE_API_KEY="):
                    YOUTUBE_API_KEY = line.strip().split("=", 1)[1]

AMAZON_ASSOCIATE_TAG = "miton31003"
AMAZON_TRACKING_ID = "business-book-ranking02-22"

YOUTUBE_API_BASE = "https://www.googleapis.com/youtube/v3"


# =============================================================================
# YouTube Data API
# =============================================================================

def api_get(endpoint, params):
    """YouTube Data API ã«GETãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    params["key"] = YOUTUBE_API_KEY
    url = f"{YOUTUBE_API_BASE}/{endpoint}?" + urllib.parse.urlencode(params)
    req = urllib.request.Request(url)
    with urllib.request.urlopen(req) as resp:
        return json.loads(resp.read().decode())


def get_uploads_playlist_id(channel_id):
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å†ç”Ÿãƒªã‚¹ãƒˆIDã‚’å–å¾—"""
    data = api_get("channels", {
        "part": "contentDetails",
        "id": channel_id,
    })
    items = data.get("items", [])
    if not items:
        return None
    return items[0]["contentDetails"]["relatedPlaylists"]["uploads"]


def get_all_video_ids(playlist_id):
    """å†ç”Ÿãƒªã‚¹ãƒˆã‹ã‚‰å…¨å‹•ç”»IDã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""
    video_ids = []
    page_token = None
    while True:
        params = {
            "part": "snippet",
            "playlistId": playlist_id,
            "maxResults": 50,
        }
        if page_token:
            params["pageToken"] = page_token
        data = api_get("playlistItems", params)
        for item in data.get("items", []):
            vid = item["snippet"]["resourceId"]["videoId"]
            video_ids.append(vid)
        page_token = data.get("nextPageToken")
        if not page_token:
            break
        time.sleep(0.1)
    return video_ids


def get_video_details(video_ids):
    """å‹•ç”»IDãƒªã‚¹ãƒˆã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆ50ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†ï¼‰"""
    videos = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        data = api_get("videos", {
            "part": "snippet,statistics",
            "id": ",".join(batch),
        })
        for item in data.get("items", []):
            snippet = item["snippet"]
            stats = item.get("statistics", {})
            videos.append({
                "video_id": item["id"],
                "title": snippet["title"],
                "published": snippet["publishedAt"],
                "link": f"https://www.youtube.com/watch?v={item['id']}",
                "summary": snippet.get("description", ""),
                "channel_title": snippet.get("channelTitle", ""),
                "view_count": int(stats.get("viewCount", 0)),
                "like_count": int(stats.get("likeCount", 0)),
            })
        time.sleep(0.1)
    return videos


def fetch_all_channel_videos(channel_id):
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®å…¨å‹•ç”»ã‚’å–å¾—"""
    playlist_id = get_uploads_playlist_id(channel_id)
    if not playlist_id:
        print(f"  [ERROR] ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å†ç”Ÿãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []
    video_ids = get_all_video_ids(playlist_id)
    print(f"  å‹•ç”»IDå–å¾—: {len(video_ids)}ä»¶")
    videos = get_video_details(video_ids)
    print(f"  å‹•ç”»è©³ç´°å–å¾—: {len(videos)}ä»¶")
    return videos


# =============================================================================
# æ›¸ç±æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
# =============================================================================

def extract_book_info_list(summary):
    """æ¦‚è¦æ¬„ã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°å†Šå¯¾å¿œï¼‰"""
    results = []

    # TODO: Amazonãƒªãƒ³ã‚¯ã‹ã‚‰æ›¸ç±æƒ…å ±ã‚’å–å¾—ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼‰
    # amazon_urls = re.findall(r'https?://amzn\.to/[A-Za-z0-9]+', summary)
    # if amazon_urls:
    #     amazon_books = extract_books_from_amazon_links(amazon_urls, max_books=5, context=summary)
    #     for book in amazon_books:
    #         results.append({
    #             "title": book["title"],
    #             "author": None,
    #             "publisher": None,
    #             "amazon_url": book["amazon_url"],
    #         })
    #     if results:
    #         return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æœ¬è¦ç´„ãƒãƒ£ãƒ³ãƒãƒ« / ã‚µãƒ©ã‚¿ãƒ¡ã•ã‚“ã€Œã‚¿ã‚¤ãƒˆãƒ«ï¼šã€ã€Œè‘—è€…ï¼šã€ã€Œå‡ºç‰ˆç¤¾ï¼šã€
    title_match = re.search(r'ã‚¿ã‚¤ãƒˆãƒ«[ï¼š:](.+)', summary)
    if title_match:
        info = {
            "title": title_match.group(1).strip(),
            "author": None,
            "publisher": None,
        }
        author_match = re.search(r'è‘—è€…[ï¼š:](.+)', summary)
        if author_match:
            info["author"] = author_match.group(1).strip()
        publisher_match = re.search(r'å‡ºç‰ˆç¤¾[ï¼š:](.+)', summary)
        if publisher_match:
            info["publisher"] = publisher_match.group(1).strip()
        results.append(info)
        return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ•ã‚§ãƒ«ãƒŸæ¼«ç”»å¤§å­¦ã€Œå‚è€ƒï¼šæ›¸å è‘—è€…å ã•ã¾ã€
    ref_match = re.search(r'å‚è€ƒ[ï¼š:](.+?)(?:\s+ã•ã¾|\s*$)', summary, re.MULTILINE)
    if ref_match:
        results.append({
            "title": ref_match.group(1).strip(),
            "author": None,
            "publisher": None,
        })
        return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: å­¦è­˜ã‚µãƒ­ãƒ³ã€Œã€amazonãƒªãƒ³ã‚¯ã€‘\nã€æ›¸åã€è‘—è€… / å‡ºç‰ˆç¤¾ã€
    if "ã€amazonãƒªãƒ³ã‚¯ã€‘" in summary:
        gakushiki_match = re.search(r'ã€(.+?)ã€(.+?)(?:\s*/\s*(.+))?$', summary, re.MULTILINE)
        if gakushiki_match:
            info = {
                "title": gakushiki_match.group(1).strip(),
                "author": None,
                "publisher": None,
            }
            if gakushiki_match.group(2):
                info["author"] = gakushiki_match.group(2).strip()
            if gakushiki_match.group(3):
                info["publisher"] = gakushiki_match.group(3).strip()
            results.append(info)
            return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ã‚µãƒ ã®æœ¬è§£èª¬chã€Œã€ä»Šå›ã®å‚è€ƒæ›¸ç±ğŸ“šã€‘ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    sam_section = re.search(
        r'ã€ä»Šå›ã®å‚è€ƒæ›¸ç±.*?ã€‘\s*\n(.*?)(?=ã€|$)', summary, re.DOTALL
    )
    if sam_section:
        section_text = sam_section.group(1).strip()
        lines = section_text.split('\n')
        title_line = None
        author_line = None
        for line in lines:
            line = line.strip()
            if not line or line.startswith('http'):
                continue
            # è‘—è€…è¡Œã‚’åˆ¤å®š: ã€Œã€œ(è‘—)ã€ã€Œã€œï¼ˆè‘—ï¼‰ã€ã‚’å«ã‚€è¡Œ
            if re.search(r'[ï¼ˆ(]è‘—[ï¼‰)]', line):
                author_line = line
            else:
                # ã‚¿ã‚¤ãƒˆãƒ«è¡Œ: ã€ŒKindleç‰ˆã€ç­‰ã‚’é™¤å»
                title_line = re.sub(r'\s*(Kindleç‰ˆ|å˜è¡Œæœ¬|æ–‡åº«|æ–°æ›¸|ãƒãƒ¼ãƒ‰ã‚«ãƒãƒ¼)\s*$', '', line).strip()
        if title_line:
            info = {"title": title_line, "author": None, "publisher": None}
            if author_line:
                author_match = re.match(r'(.+?)\s*[ï¼ˆ(]è‘—[ï¼‰)]', author_line)
                if author_match:
                    info["author"] = author_match.group(1).strip()
                pub_match = re.search(r'([^\s]+?)[ï¼ˆ(]ç·¨é›†[ï¼‰)]', author_line)
                if pub_match:
                    info["publisher"] = pub_match.group(1).strip()
            results.append(info)
            return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³5: PIVOTã€Œï¼œå‚è€ƒæ›¸ç±ï¼ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    pivot_section = re.search(
        r'[ï¼œ<]å‚è€ƒæ›¸ç±[ï¼>]\s*\n(.*?)(?=\n[ï¼œ<]|\nâ€»|$)', summary, re.DOTALL
    )
    if pivot_section:
        section_text = pivot_section.group(1).strip()
        lines = section_text.split('\n')
        for line in lines:
            line = line.strip()
            if not line or line.startswith('http') or line.startswith('â€»'):
                continue

            title = None
            author = None

            # ãƒ‘ã‚¿ãƒ¼ãƒ³A: ã€ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚’å„ªå…ˆï¼ˆå†…éƒ¨ã«ã€Œã€ãŒå«ã¾ã‚Œã¦ã‚‚OKï¼‰
            book_match = re.search(r'ã€(.+?)ã€', line)
            if book_match:
                title = book_match.group(1).strip()
                before = line[:book_match.start()].strip()
                if before:
                    author = before
                after = line[book_match.end():].strip()
                if not author and after:
                    a_match = re.match(r'(.+?)\s*[ï¼ˆ(]è‘—[ï¼‰)]', after)
                    if a_match:
                        author = a_match.group(1).strip()

            # ãƒ‘ã‚¿ãƒ¼ãƒ³B: ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ï¼‹å¾Œç¶šãƒ†ã‚­ã‚¹ãƒˆã‚‚å«ã‚ã‚‹
            if not title:
                book_match = re.search(r'ã€Œ(.+?)ã€(.+?)(?=[ï¼ˆ(]|https?://|\s*$)', line)
                if book_match:
                    # ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ã®å¾Œã‚ã‚‚ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸€éƒ¨ã¨ã—ã¦çµåˆ
                    title = book_match.group(1).strip() + book_match.group(2).strip()
                    # æœ«å°¾ã®æ‹¬å¼§å†…ï¼ˆå‡ºç‰ˆç¤¾ç­‰ï¼‰ã‚’é™¤å»
                    title = re.sub(r'[ï¼ˆ(][^ï¼‰)]+[ï¼‰)]$', '', title).strip()

            if not title:
                continue

            results.append({
                "title": title,
                "author": author,
                "publisher": None,
            })
        # PIVOTã®å‚è€ƒæ›¸ç±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹å ´åˆã¯çµæœã«é–¢ã‚ã‚‰ãšã“ã“ã§è¿”ã™
        # ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³6ã®amzn.toæ±ç”¨æŠ½å‡ºã«è½ã¡ãªã„ã‚ˆã†ã«ã™ã‚‹ï¼‰
        return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³6: ä¸ƒç€¬ã‚¢ãƒªãƒ¼ã‚µ â€” amzn.toãƒªãƒ³ã‚¯ã‹ã‚‰æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
    # å½¢å¼A: ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€€https://amzn.to/xxxã€(åŒä¸€è¡Œ)
    # å½¢å¼B: ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€+ æ¬¡è¡Œã€Œhttps://amzn.to/xxxã€(åˆ¥è¡Œ)
    amazon_lines = re.findall(r'https?://amzn\.to/[A-Za-z0-9]+', summary)
    if amazon_lines:
        lines = summary.split('\n')
        ng_words = ['Amazon', 'URL', 'ãƒªãƒ³ã‚¯', 'ä¸ƒç€¬', 'å•†å“ç´¹ä»‹', 'ç‰¹å…¸',
                    'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚«ãƒ¼ãƒ‰', 'Success Book', 'å‹•ç”»', 'æ¦‚è¦æ¬„',
                    'ãŠã™ã™ã‚é †ã§ã¯ãªã„', 'ã‚¢ã‚½ã‚·ã‚¨ã‚¤ãƒˆ', 'è³¼å…¥ãƒšãƒ¼ã‚¸',
                    'æä¾›:', 'Mainichi Eikaiwa', 'è©•åˆ¤', 'ãŠã™ã™ã‚æœ¬', 'å‡ºæ¼”æœ¬',
                    'å‚è€ƒæœ¬', 'ãŠå‹§ã‚æœ¬', 'TOEIC', 'å‹‰å¼·æœ¬', 'ã‚ªãƒ¼ãƒ‡ã‚£ãƒ–ãƒ«',
                    'Audible', 'Kindle', 'Udemy', 'æ‰‹å¸³', 'ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼',
                    'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è‹±ä¼šè©±', 'AQUES', 'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²', 'LOWYAã®',
                    'Meta Quest', 'Kindleç«¯æœ«', 'æœ¬æ£šãƒ‡ã‚¹ã‚¯', 'ã¯ã“ã¡ã‚‰',
                    'ã‚¿ã‚¤ãƒãƒ¼', 'ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼', 'ãƒœãƒ¼ãƒ‰ã‚²ãƒ¼ãƒ ', 'ã‹ã£ã•',
                    'ãƒ†ãƒ©ãƒ˜ãƒ«ãƒ„', 'ã‚¤ãƒ¤ãƒ›ãƒ³', 'ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰', 'ãƒã‚¦ã‚¹',
                    'ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤', 'ãƒ¢ãƒ‹ã‚¿ãƒ¼', 'ãƒã‚§ã‚¢', 'ãƒ©ã‚¤ãƒˆä»˜ã',
                    'é‡‘ãƒ•ãƒ¬', 'ã‚­ã‚¯ã‚¿ãƒ³', 'ã§ã‚‹1000å•', 'å…¬å¼å•é¡Œé›†',
                    'ç²¾é¸å•é¡Œé›†', 'ç²¾é¸æ¨¡è©¦']

        for i, line in enumerate(lines):
            line_stripped = line.strip()
            amazon_match = re.search(r'https?://amzn\.to/[A-Za-z0-9]+', line_stripped)
            if not amazon_match:
                continue

            title_candidate = None
            amazon_url = amazon_match.group(0)

            # å½¢å¼A: amzn.toã®å‰ã«ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹ï¼ˆåŒä¸€è¡Œï¼‰
            before_url = line_stripped[:amazon_match.start()].strip()
            if before_url and not before_url.startswith('http'):
                title_candidate = before_url
            # å½¢å¼B: amzn.toã ã‘ã®è¡Œ â†’ å‰ã®è¡ŒãŒã‚¿ã‚¤ãƒˆãƒ«
            elif line_stripped == amazon_url and i > 0:
                prev_line = lines[i-1].strip()
                if prev_line and not prev_line.startswith('http'):
                    title_candidate = prev_line

            if not title_candidate:
                continue

            # NGãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            if any(ng in title_candidate for ng in ng_words):
                continue

            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            cleaned = re.sub(r'^[*\sãƒ»â€»â¤ï¸ğŸ“•ğŸ“—ğŸ“˜ğŸ“™ğŸ”½â–½â†“]+', '', title_candidate).strip()
            # æ‹¬å¼§ä»˜ãã®è£œè¶³ã‚’é™¤å»: ã€Œã‚¿ã‚¤ãƒˆãƒ«(Amazon)ã€â†’ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€
            cleaned = re.sub(r'[ï¼ˆ(](?:Amazon|Amazonãƒªãƒ³ã‚¯|ã‚¢ãƒã‚¾ãƒ³)[ï¼‰)]$', '', cleaned).strip()
            # ã€ã€ã€Œã€ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯å¤–ã™
            if cleaned.startswith('ã€') and cleaned.endswith('ã€'):
                cleaned = cleaned[1:-1]
            if cleaned.startswith('ã€Œ') and cleaned.endswith('ã€'):
                cleaned = cleaned[1:-1]

            if cleaned and len(cleaned) > 2:
                results.append({
                    "title": cleaned,
                    "author": None,
                    "publisher": None,
                })

        if results:
            return results

    # ãƒ‘ã‚¿ãƒ¼ãƒ³5: ã‚¢ãƒã‚¿ãƒ­ãƒ¼ã€Œæ›¸ç±ã®è³¼å…¥ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    abataro_section = re.search(
        r'(?:ã€æ›¸ç±ã®è³¼å…¥ã€‘|â–¼æ›¸ç±ã®è³¼å…¥)\s*\n?(.*?)(?=\nâ–¼|\n\n\n|\Z)', summary, re.DOTALL
    )
    if abataro_section:
        section_text = abataro_section.group(1)
        lines = section_text.strip().split('\n')
        seen_titles = set()
        is_first = True
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            # éæ›¸ç±è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if not line or line.startswith('http') or 'ã‚¨ãƒƒã‚»ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ' in line or 'ç°¡æ˜“ç‰ˆ' in line:
                i += 1
                continue
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ã‚µãƒ¼ãƒ“ã‚¹å®£ä¼ãƒ»ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãƒ»çµµæ–‡å­—ä»˜ãå‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if (line.startswith('ã€') or line.startswith('#') or
                'Audible' in line or 'Kindle' in line or 'amzn.to' in line or
                line.startswith('ğŸ“—') or line.startswith('ğŸ“•') or
                'æœ¬ã‚’è´ã' in line or 'é–¢é€£å‹•ç”»' in line or
                'åˆ†è§£èª¬' in line or 'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²' in line or
                'SNS' in line or 'Twitter' in line or 'Instagram' in line or
                'OUTPUTèª­æ›¸è¡“' in line):
                i += 1
                continue
            line = re.sub(r'^ãƒ»\s*', '', line)
            book_match = re.match(r'(.+?)(?:[ï½œ|](.+?))?(?:[ï¼ˆ(](.+?)[ï¼‰)])?$', line)
            if book_match:
                title = book_match.group(1).strip()
                author = book_match.group(2).strip() if book_match.group(2) else None
                publisher = book_match.group(3).strip() if book_match.group(3) else None
                if title not in seen_titles:
                    seen_titles.add(title)
                    results.append({
                        "title": title,
                        "author": author,
                        "publisher": publisher,
                        "_is_first": is_first,
                    })
                is_first = False
            i += 1
        if results:
            return results

    return results


def is_valid_book_title(title):
    """æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦æœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    if not title or not isinstance(title, str):
        return False

    title = title.strip()

    # çŸ­ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’é™¤å¤–ï¼ˆ3æ–‡å­—ä»¥ä¸‹ï¼‰
    if len(title) <= 3:
        return False

    # çµµæ–‡å­—ã§å§‹ã¾ã‚‹ã‚‚ã®ã‚’é™¤å¤–
    emoji_starts = ['ğŸ“š', 'ğŸ“—', 'ğŸ“•', 'ğŸ“˜', 'ğŸ“™', 'â–¼', 'ã€', 'â– ', 'â—', 'ãƒ»', 'â€»']
    if any(title.startswith(emoji) for emoji in emoji_starts):
        return False

    # NGãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã‚„å®£ä¼ï¼‰ã‚’é™¤å¤–
    ng_words = [
        'ãã®ä»–',
        'ãŠã™ã™ã‚å‹•ç”»',
        'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²',
        'é–¢é€£å‹•ç”»',
        'å‹•ç”»ä¸€è¦§',
        'SNS',
        'Twitter',
        'Instagram',
        'LINE',
        'ã‚¨ãƒƒã‚»ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ',
        'ç°¡æ˜“ç‰ˆ',
        'Audible',
        'Kindle',
        'æœ¬ã‚’è´ã',
        'åˆ†è§£èª¬',
        'è¦ç´„',
        'è§£èª¬',
        'ã¾ã¨ã‚',
        'ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ',
        'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³',
        'ç„¡æ–™',
        'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«',
        'ãŠå•ã„åˆã‚ã›',
        'ãƒ¡ãƒ³ãƒãƒ¼ã‚·ãƒƒãƒ—',
        'ã‚µãƒ–ãƒãƒ£ãƒ³ãƒãƒ«',
        # ä¸ƒç€¬ã‚¢ãƒªãƒ¼ã‚µé–¢é€£ã®å®£ä¼ã‚’é™¤å¤–
        'ä¸ƒç€¬åˆ¶ä½œ',
        'å•†å“ç´¹ä»‹',
        'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚«ãƒ¼ãƒ‰',
        'Success Book',
        'Your Success',
        'è³¼å…¥ãƒšãƒ¼ã‚¸',
        'ç‰¹å…¸',
        'ãŠã™ã™ã‚é †ã§ã¯ãªã„',
        'æ¦‚è¦æ¬„',
        'ãƒ‡ã‚¸ã‚¿ãƒ«ç‰ˆ',
        'å†Šå­ç‰ˆ',
        # YouTuberè‡ªè‘—ã®å®£ä¼ã‚’é™¤å¤–
        'OUTPUTèª­æ›¸è¡“',
    ]

    for ng in ng_words:
        if ng in title:
            return False

    # ã€Œæœ¬ã€ã ã‘ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’é™¤å¤–
    if title in ['æœ¬', 'æ›¸ç±', 'å›³æ›¸', 'book', 'books']:
        return False

    # URLã£ã½ã„ã‚‚ã®ã‚’é™¤å¤–
    if 'http' in title.lower() or '.com' in title.lower():
        return False

    # å…¨ã¦è¨˜å·ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’é™¤å¤–
    if all(not c.isalnum() for c in title):
        return False

    # YouTuberåãŒå…¥ã£ã¦ã„ã‚‹ã‚‚ã®ã‚’é™¤å¤–ï¼ˆè‡ªè‘—å®£ä¼ã®å¯èƒ½æ€§ï¼‰
    youtuber_names = ['ã‚¢ãƒã‚¿ãƒ­ãƒ¼', 'ã‚µãƒ©ã‚¿ãƒ¡', 'æœ¬è¦ç´„ãƒãƒ£ãƒ³ãƒãƒ«', 'å­¦è­˜ã‚µãƒ­ãƒ³', 'ãƒ•ã‚§ãƒ«ãƒŸ', 'ä¸‰å®…', 'ä¸ƒç€¬', 'ã‚¢ãƒªãƒ¼ã‚µ']
    for name in youtuber_names:
        if name in title:
            return False

    return True


def generate_amazon_search_url(book_title):
    """æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰Amazonæ¤œç´¢URLã‚’ç”Ÿæˆï¼ˆã‚¢ã‚½ã‚·ã‚¨ã‚¤ãƒˆã‚¿ã‚°ä»˜ãï¼‰"""
    query = urllib.parse.quote(book_title)
    return f"https://www.amazon.co.jp/s?k={query}&i=stripbooks&tag={AMAZON_TRACKING_ID}"


def generate_book_id(title):
    """æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ç”Ÿæˆ"""
    return hashlib.md5(title.encode()).hexdigest()[:12]


# =============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =============================================================================

def load_channels():
    with open(CHANNELS_FILE, "r", encoding="utf-8") as f:
        return json.load(f)["channels"]


def main():
    if not YOUTUBE_API_KEY:
        print("ERROR: YOUTUBE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    os.makedirs(DATA_DIR, exist_ok=True)
    channels = load_channels()
    all_books = {}

    for ch in channels:
        channel_name = ch["name"]
        channel_id = ch["channel_id"]
        print(f"\n=== {channel_name} (ID: {channel_id}) ===")

        videos = fetch_all_channel_videos(channel_id)

        for video in videos:
            summary = video.get("summary", "")
            book_info_list = extract_book_info_list(summary)

            if not book_info_list:
                continue

            for book_info in book_info_list:
                book_title = book_info.get("title")
                if not book_title:
                    continue

                # ã‚¿ã‚¤ãƒˆãƒ«ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                if not is_valid_book_title(book_title):
                    continue

                # è‡ªè‘—å®£ä¼ã‚¹ã‚­ãƒƒãƒ—
                if book_info.get("_is_first") and len(book_info_list) > 1:
                    continue

                # Amazonãƒªãƒ³ã‚¯ã‹ã‚‰å–å¾—ã—ãŸå ´åˆã¯æ—¢ã«amazon_urlãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹
                amazon_url = book_info.get("amazon_url") or generate_amazon_search_url(book_title)

                if book_title not in all_books:
                    all_books[book_title] = {
                        "id": generate_book_id(book_title),
                        "title": book_title,
                        "author": book_info.get("author"),
                        "publisher": book_info.get("publisher"),
                        "amazon_url": amazon_url,
                        "count": 0,
                        "total_views": 0,
                        "total_likes": 0,
                        "videos": [],
                    }

                all_books[book_title]["count"] += 1
                all_books[book_title]["total_views"] += video.get("view_count", 0)
                all_books[book_title]["total_likes"] += video.get("like_count", 0)
                all_books[book_title]["videos"].append({
                    "video_id": video["video_id"],
                    "video_title": video["title"],
                    "channel": channel_name,
                    "link": video["link"],
                    "published": video["published"],
                    "view_count": video.get("view_count", 0),
                    "like_count": video.get("like_count", 0),
                })

    # --- çµæœè¡¨ç¤º ---
    books_list = list(all_books.values())
    print(f"\n=== æŠ½å‡ºçµæœ ===")
    print(f"æ›¸ç±æ•°: {len(books_list)}")

    # --- JSONç”Ÿæˆ ---

    # books.jsonï¼ˆç´¹ä»‹å›æ•°é †ï¼‰
    books_by_count = sorted(books_list, key=lambda x: x["count"], reverse=True)
    with open(os.path.join(DATA_DIR, "books.json"), "w", encoding="utf-8") as f:
        json.dump(books_by_count, f, ensure_ascii=False, indent=2)

    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”¨ã®è»½é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    def make_ranking_entry(book):
        return {
            "id": book["id"],
            "title": book["title"],
            "author": book.get("author"),
            "count": book["count"],
            "total_views": book["total_views"],
            "total_likes": book["total_likes"],
            "amazon_url": book["amazon_url"],
        }

    # rankings.jsonï¼ˆç´¹ä»‹å›æ•°é †ï¼‰
    rankings_count = [make_ranking_entry(b) for b in books_by_count]
    with open(os.path.join(DATA_DIR, "rankings.json"), "w", encoding="utf-8") as f:
        json.dump(rankings_count, f, ensure_ascii=False, indent=2)

    # rankings_views.jsonï¼ˆå†ç”Ÿå›æ•°åˆè¨ˆé †ï¼‰
    books_by_views = sorted(books_list, key=lambda x: x["total_views"], reverse=True)
    rankings_views = [make_ranking_entry(b) for b in books_by_views]
    with open(os.path.join(DATA_DIR, "rankings_views.json"), "w", encoding="utf-8") as f:
        json.dump(rankings_views, f, ensure_ascii=False, indent=2)

    # rankings_likes.jsonï¼ˆã„ã„ã­åˆè¨ˆé †ï¼‰
    books_by_likes = sorted(books_list, key=lambda x: x["total_likes"], reverse=True)
    rankings_likes = [make_ranking_entry(b) for b in books_by_likes]
    with open(os.path.join(DATA_DIR, "rankings_likes.json"), "w", encoding="utf-8") as f:
        json.dump(rankings_likes, f, ensure_ascii=False, indent=2)

    print(f"\n--- TOP20ï¼ˆç´¹ä»‹å›æ•°é †ï¼‰---")
    for i, book in enumerate(books_by_count[:20], 1):
        print(f"  {i}. ã€{book['title']}ã€ (ç´¹ä»‹{book['count']}å› / å†ç”Ÿ{book['total_views']:,} / ã„ã„ã­{book['total_likes']:,})")

    print(f"\n--- TOP10ï¼ˆå†ç”Ÿå›æ•°é †ï¼‰---")
    for i, book in enumerate(books_by_views[:10], 1):
        print(f"  {i}. ã€{book['title']}ã€ (å†ç”Ÿ{book['total_views']:,} / ç´¹ä»‹{book['count']}å›)")

    print(f"\n--- TOP10ï¼ˆã„ã„ã­é †ï¼‰---")
    for i, book in enumerate(books_by_likes[:10], 1):
        print(f"  {i}. ã€{book['title']}ã€ (ã„ã„ã­{book['total_likes']:,} / ç´¹ä»‹{book['count']}å›)")

    print(f"\nãƒ‡ãƒ¼ã‚¿ã‚’ {DATA_DIR} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()
